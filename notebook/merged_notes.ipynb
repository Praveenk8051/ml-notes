{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Regression\n",
    "\n",
    "Regression assumes a relationship between features (input) and response (output). The objective is to learn a function $ f(X) $ that maps input data to continuous outputs.\n",
    "\n",
    "**Examples:**\n",
    "- Predicting salary based on education and experience.\n",
    "- Estimating house price based on square footage and number of rooms.\n",
    "\n",
    "1. [Simple Linear Regression](#simple-linear-regression)\n",
    "2. [Multiple Regression Model](#multiple-regression-model)\n",
    "3. [Polynomial Regression](#polynomial-regression)\n",
    "4. [Feature Engineering & Extraction](#feature-engineering--extraction)\n",
    "5. [Matrix Notation for Multiple Regression](#matrix-notation-for-multiple-regression)\n",
    "6. [Model Fitting](#model-fitting)\n",
    "7. [Gradient Descent for Multiple Regression](#gradient-descent-for-multiple-regression)\n",
    "8. [Coefficient Interpretation](#coefficient-interpretation)\n",
    "9. [Evaluating Regression Models](#evaluating-regression-models)\n",
    "    - [Loss Functions: Measuring Error](#1-loss-functions-measuring-error)\n",
    "    - [Training vs. Generalization Error](#2-training-vs-generalization-error)\n",
    "    - [Overfitting vs. Underfitting](#3-overfitting-vs-underfitting)\n",
    "    - [Test Error: Estimating Generalization](#4-test-error-estimating-generalization)\n",
    "    - [Bias-Variance Tradeoff](#5-bias-variance-tradeoff)\n",
    "    - [Error Decomposition: Three Sources of Error](#6-error-decomposition-three-sources-of-error)\n",
    "    - [Training, Validation, and Test Sets](#7-training-validation-and-test-sets)\n",
    "    - [Model Selection Using a Validation Set](#8-model-selection-using-a-validation-set)\n",
    "    - [Cross-Validation (When Data is Limited)](#9-cross-validation-when-data-is-limited)\n",
    "10. [Overfitting & Bias-Variance Tradeoff](#overfitting--bias-variance-tradeoff)\n",
    "11. [Ridge Regression (L2 Regularization)](#ridge-regression-l2-regularization)\n",
    "12. [Algorithm for Ridge Regression](#algorithm-for-ridge-regression)\n",
    "13. [Cross-Validation for Choosing $ \\lambda $](#cross-validation-for-choosing--lambda-)\n",
    "14. [Handling the Intercept Term](#handling-the-intercept-term)\n",
    "15. [Feature Selection in Regression](#feature-selection-in-regression)\n",
    "    - [Explicit vs. Implicit Feature Selection](#explicit-vs-implicit-feature-selection)\n",
    "    - [All Subsets Feature Selection](#1-all-subsets-feature-selection)\n",
    "    - [Greedy Algorithms for Feature Selection](#2-greedy-algorithms-for-feature-selection)\n",
    "    - [Regularization-Based Feature Selection (Lasso Regression)](#3-regularization-based-feature-selection-lasso-regression)\n",
    "    - [Optimization of Lasso: Coordinate Descent](#4-optimization-of-lasso-coordinate-descent)\n",
    "16. [Nonparametric Regression Approaches](#nonparametric-regression-approaches)\n",
    "    - [Local vs. Global Fits](#local-vs-global-fits)\n",
    "    - [K-Nearest Neighbors (k-NN) Regression](#k-nearest-neighbors-k-nn-regression)\n",
    "    - [Weighted k-NN and Kernel Regression](#weighted-k-nn-and-kernel-regression)\n",
    "    - [Locally Weighted Regression](#locally-weighted-regression)\n",
    "17. [Theoretical and Practical Aspects](#theoretical-and-practical-aspects)\n",
    "    - [k-NN for Classification](#k-nn-for-classification)\n",
    "\n",
    "# Simple Linear Regression\n",
    "\n",
    "**Definition:** Models a relationship between a single input feature and output using a straight line:\n",
    "$Y = w_0 + w_1 X + \\epsilon$\n",
    "where:\n",
    "- $ w_0 $ is the intercept.\n",
    "- $ w_1 $ is the slope.\n",
    "- $ \\epsilon $ represents error (residuals).\n",
    "\n",
    "**Goal:** Find $ w_0 $ and $ w_1 $ that best fit the data.\n",
    "\n",
    "**Optimization:** Minimize Residual Sum of Squares (RSS).\n",
    "\n",
    "# Multiple Regression Model\n",
    "\n",
    "**What is Multiple Regression?**\n",
    "Instead of just one input feature ($ X $), we use multiple features $ X_1, X_2, \\ldots, X_d $.\n",
    "\n",
    "**General form:**\n",
    "$ Y = w_0 + w_1 X_1 + w_2 X_2 + \\ldots + w_d X_d + \\epsilon $\n",
    "where:\n",
    "- $ w_0 $ is the intercept.\n",
    "- $ w_1, w_2, \\ldots $ are regression coefficients.\n",
    "- $ \\epsilon $ is the error term.\n",
    "\n",
    "# Polynomial Regression\n",
    "\n",
    "Extends linear regression by including higher-order terms of a single feature.\n",
    "\n",
    "**Example:** Instead of modeling $ Y = w_0 + w_1 X $, we add powers of $ X $, like:\n",
    "$ Y = w_0 + w_1 X + w_2 X^2 + w_3 X^3 + \\cdots + w_p X^p + \\epsilon $\n",
    "This allows the model to capture curvature.\n",
    "\n",
    "# Feature Engineering & Extraction\n",
    "\n",
    "Transforming Inputs into Features\n",
    "Features don’t always have to be raw inputs. They can be functions of inputs.\n",
    "\n",
    "**Example:**\n",
    "Instead of using just $ X $ (square footage), use:\n",
    "$ X, X^2 $ (quadratic), $ \\sin(X) $, $ \\log(X) $, etc.\n",
    "\n",
    "**Seasonality Example (Time-Series Modeling):**\n",
    "Housing prices often fluctuate seasonally. We can model this with sinusoidal features:\n",
    "$ Y = w_0 + w_1 T + w_2 \\sin\\left(\\frac{2\\pi T}{12}\\right) + w_3 \\cos\\left(\\frac{2\\pi T}{12}\\right) + \\epsilon $\n",
    "This accounts for cyclical trends, e.g., higher house prices in summer.\n",
    "\n",
    "**Applications Beyond Housing:**\n",
    "- Weather Forecasting: Uses multiple seasonal patterns (daily, yearly).\n",
    "- Flu Monitoring: Flu cases rise and fall seasonally.\n",
    "- E-Commerce: Seasonal demand forecasting for products (e.g., winter coats).\n",
    "\n",
    "# Matrix Notation for Multiple Regression\n",
    "\n",
    "To handle multiple variables efficiently, we rewrite our equations using matrices.\n",
    "\n",
    "**Input Matrix (H):** Rows = Observations, Columns = Features.\n",
    "**Weights Vector (W):** Contains regression coefficients.\n",
    "**Output Vector (Y):** Contains observed values.\n",
    "\n",
    "The multiple regression model in matrix form:\n",
    "$ Y = HW + \\epsilon $\n",
    "where:\n",
    "- $ H $ is the design matrix.\n",
    "- $ W $ is the vector of regression coefficients.\n",
    "- $ \\epsilon $ is the error vector.\n",
    "\n",
    "# Model Fitting\n",
    "\n",
    "**Closed-Form Solution (Normal Equation):**\n",
    "Deriving the best weights by setting the gradient of Residual Sum of Squares (RSS) to zero. The optimal weights are given by:\n",
    "$ W = (H^T H)^{-1} H^T Y $\n",
    "\n",
    "**Pros:**\n",
    "- Exact solution.\n",
    "- Works well for small datasets.\n",
    "\n",
    "**Cons:**\n",
    "- Computationally expensive ($ O(d^3) $ complexity).\n",
    "- Requires matrix inversion, which may be unstable.\n",
    "\n",
    "# Gradient Descent for Multiple Regression\n",
    "\n",
    "Alternative to matrix inversion for large datasets. Iteratively updates weights using:\n",
    "$ W(t+1) = W(t) - \\eta \\nabla RSS $\n",
    "where:\n",
    "- $ \\eta $ is the learning rate.\n",
    "- $ \\nabla RSS $ is the gradient:\n",
    "$ \\nabla RSS = -2H^T (Y - HW) $\n",
    "\n",
    "**Intuition:**\n",
    "- If the model underestimates the effect of a feature, its coefficient increases.\n",
    "- If it overestimates, the coefficient decreases.\n",
    "\n",
    "**Pros:**\n",
    "- Works for large feature spaces.\n",
    "- Avoids expensive matrix inversion.\n",
    "\n",
    "**Cons:**\n",
    "- Requires careful tuning of learning rate.\n",
    "- Can get stuck in local minima (though less of a problem for regression).\n",
    "\n",
    "# Coefficient Interpretation\n",
    "\n",
    "The coefficient $ w_j $ represents the impact of feature $ X_j $ on the output when all other features are held constant.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Housing price model with square footage and number of bathrooms:\n",
    "$ Y = w_0 + w_1 (\\text{square footage}) + w_2 (\\text{bathrooms}) + \\epsilon $\n",
    "\n",
    "- $ w_1 $ = How much the price changes per additional square foot, holding bathrooms constant.\n",
    "- $ w_2 $ = How much price changes when adding a bathroom, assuming square footage remains fixed.\n",
    "\n",
    "**Caution:**\n",
    "\n",
    "Coefficients depend on the context of the model. If we omit important variables, coefficients might be misleading.\n",
    "\n",
    "# Evaluating Regression Models\n",
    "\n",
    "## 1. Loss Functions: Measuring Error\n",
    "\n",
    "In machine learning, we define a loss function to measure how bad our predictions are.\n",
    "\n",
    "### Types of Loss Functions\n",
    "\n",
    "1. **Absolute Error (L1 Loss)**\n",
    "$ L(y, \\hat{y}) = |y - \\hat{y}| $\n",
    "Measures the absolute difference between actual and predicted values.\n",
    "Less sensitive to outliers compared to squared error.\n",
    "\n",
    "2. **Squared Error (L2 Loss)**\n",
    "$ L(y, \\hat{y}) = (y - \\hat{y})^2 $\n",
    "Penalizes large errors more than absolute error.\n",
    "Leads to models that prioritize minimizing large deviations.\n",
    "\n",
    "## 2. Training vs. Generalization Error\n",
    "\n",
    "### Training Error\n",
    "\n",
    "The error computed on the same data the model was trained on.\n",
    "Formula:\n",
    "$ \\text{Training Error} = \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, \\hat{y}_i) $\n",
    "Problem? It’s often too optimistic since the model has already seen this data.\n",
    "\n",
    "### Generalization Error\n",
    "\n",
    "The true error on unseen data.\n",
    "Cannot be computed exactly because we don’t have access to all possible future data.\n",
    "Goal: Find a good approximation using test error.\n",
    "\n",
    "## 3. Overfitting vs. Underfitting\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "Model fits training data too well, capturing noise instead of real patterns.\n",
    "Symptoms:\n",
    "- Very low training error, but high test error.\n",
    "- Poor performance on new data.\n",
    "Cause: Model is too complex.\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "Model is too simple to capture patterns in data.\n",
    "Symptoms:\n",
    "- High training and test error.\n",
    "- Fails to represent underlying trends.\n",
    "Cause: Model lacks capacity.\n",
    "\n",
    "## 4. Test Error: Estimating Generalization\n",
    "\n",
    "### Why Do We Need Test Error?\n",
    "\n",
    "Since generalization error is unknown, we approximate it using test error:\n",
    "$ \\text{Test Error} = \\frac{1}{N_{\\text{test}}} \\sum_{i=1}^{N_{\\text{test}}} L(y_i, \\hat{y}_i) $\n",
    "Key Property: Test data must be completely unseen by the model.\n",
    "\n",
    "## 5. Bias-Variance Tradeoff\n",
    "\n",
    "Machine learning models suffer from two main types of errors:\n",
    "\n",
    "### Bias (Error due to assumptions)\n",
    "\n",
    "If a model is too simple, it makes strong assumptions → High Bias.\n",
    "Example: Linear model trying to fit non-linear data.\n",
    "\n",
    "### Variance (Error due to sensitivity)\n",
    "\n",
    "If a model is too complex, it becomes sensitive to training data variations → High Variance.\n",
    "Example: High-degree polynomial regression fitting noise.\n",
    "\n",
    "### Tradeoff\n",
    "\n",
    "Low Bias + Low Variance = Ideal Model (but difficult to achieve).\n",
    "We aim for a balance between bias and variance.\n",
    "\n",
    "## 6. Error Decomposition: Three Sources of Error\n",
    "\n",
    "For any given prediction, total error can be decomposed into:\n",
    "$ \\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error} $\n",
    "- Bias: Error due to model assumptions.\n",
    "- Variance: Sensitivity to training data.\n",
    "- Irreducible Error: Noise inherent in data (cannot be eliminated).\n",
    "Goal: Find the right model complexity that balances bias and variance.\n",
    "\n",
    "## 7. Training, Validation, and Test Sets\n",
    "\n",
    "To properly evaluate models, we split data into three sets:\n",
    "\n",
    "1. **Training Set**\n",
    "   Used to train the model.\n",
    "\n",
    "2. **Validation Set**\n",
    "   Used to tune hyperparameters (e.g., degree of polynomial).\n",
    "   Helps in model selection.\n",
    "\n",
    "3. **Test Set**\n",
    "   Completely untouched data.\n",
    "   Used for final performance assessment.\n",
    "\n",
    "## 8. Model Selection Using a Validation Set\n",
    "\n",
    "### Why Not Use Test Data for Model Selection?\n",
    "\n",
    "If we optimize a model based on test data, it’s no longer a fair estimate of generalization.\n",
    "Solution? Introduce a validation set:\n",
    "- Train models with different complexities.\n",
    "- Select the best model based on validation error.\n",
    "- Finally, evaluate on the test set.\n",
    "\n",
    "## 9. Cross-Validation (When Data is Limited)\n",
    "\n",
    "When we don’t have enough data to split into training, validation, and test sets, we use cross-validation.\n",
    "\n",
    "### K-Fold Cross-Validation:\n",
    "\n",
    "- Split data into K subsets (folds).\n",
    "- Train on K-1 folds, test on 1 fold.\n",
    "- Repeat K times and average results.\n",
    "\n",
    "# Overfitting & Bias-Variance Tradeoff\n",
    "\n",
    "Complex models (like high-degree polynomials) have low bias but high variance, making them sensitive to training data. Simpler models have high bias but low variance and fail to capture underlying patterns. The goal is to balance bias and variance for optimal predictive performance.\n",
    "\n",
    "**Example: Polynomial Regression**\n",
    "- A quadratic fit may capture general trends.\n",
    "- A 16-degree polynomial will overfit, showing extreme fluctuations.\n",
    "- When overfitting occurs, the magnitude of model coefficients increases significantly.\n",
    "\n",
    "# Ridge Regression (L2 Regularization)\n",
    "\n",
    "Ridge regression penalizes large coefficients by adding an L2 norm term to the cost function.\n",
    "\n",
    "The cost function is modified to:\n",
    "$ RSS(w) + \\lambda \\sum w_j^2 $\n",
    "\n",
    "Where:\n",
    "- RSS(w) = Residual Sum of Squares (fit to data)\n",
    "- $ \\lambda $ (lambda) = Regularization parameter controlling penalty strength\n",
    "\n",
    "**Effect of $ \\lambda $:**\n",
    "- Large $ \\lambda $ → Shrinks coefficients (reducing complexity)\n",
    "- Small $ \\lambda $ → Similar to Least Squares Regression (can overfit)\n",
    "- $ \\lambda = 0 $ → Regular least squares regression\n",
    "- $ \\lambda \\to \\infty $ → All coefficients approach 0 (very high bias, underfitting)\n",
    "- $ 0 < \\lambda < \\infty $ → Balanced complexity\n",
    "\n",
    "# Algorithm for Ridge Regression\n",
    "\n",
    "**Closed-Form Solution**\n",
    "The ridge regression parameters can be computed using:\n",
    "$ w_{ridge} = (H^T H + \\lambda I)^{-1} H^T y $\n",
    "\n",
    "Where:\n",
    "- $ H $ = Feature matrix\n",
    "- $ I $ = Identity matrix\n",
    "- $ \\lambda I $ ensures the inverse exists, even when features are highly correlated or there are more features than samples.\n",
    "\n",
    "**Gradient Descent Formulation**\n",
    "Ridge regression can also be solved iteratively using gradient descent.\n",
    "\n",
    "The weight update rule is:\n",
    "$ w_j(t+1) = (1 - 2 \\eta \\lambda) w_j(t) - \\eta \\frac{\\partial RSS}{\\partial w_j} $\n",
    "\n",
    "- The first term shrinks the coefficient.\n",
    "- The second term updates based on the gradient of RSS.\n",
    "\n",
    "# Cross-Validation for Choosing $ \\lambda $\n",
    "\n",
    "$ \\lambda $ must be tuned automatically to avoid manually testing different values.\n",
    "\n",
    "**K-Fold Cross-Validation:**\n",
    "- The dataset is split into K equal parts.\n",
    "- Each subset is used as a validation set while the others are used for training.\n",
    "- The error across all folds is averaged to find the best $ \\lambda $.\n",
    "\n",
    "**Leave-One-Out Cross-Validation (LOOCV):**\n",
    "- Uses each data point one at a time as a validation set.\n",
    "- More accurate, but computationally expensive.\n",
    "\n",
    "# Handling the Intercept Term\n",
    "\n",
    "The ridge penalty shrinks all coefficients, including the intercept $ w_0 $.\n",
    "\n",
    "**Two possible solutions:**\n",
    "1. Exclude the intercept from regularization by modifying the identity matrix.\n",
    "2. Center the data around zero before applying ridge regression (common in practice).\n",
    "\n",
    "# Feature Selection in Regression\n",
    "\n",
    "Feature selection is crucial for efficiency, interpretability, and improving model performance. It helps in:\n",
    "\n",
    "- Reducing computation: If we have an extremely large feature set (e.g., 100 billion features), computation becomes infeasible.\n",
    "- Improving interpretability: Understanding which features matter most (e.g., in house pricing, not every tiny detail, like having a microwave, is important).\n",
    "\n",
    "## Explicit vs. Implicit Feature Selection\n",
    "\n",
    "- **Explicit methods:** Search for the best subset of features using algorithms like all subsets selection and greedy methods.\n",
    "- **Implicit methods:** Use regularization techniques like Lasso Regression to automatically shrink coefficients and perform feature selection.\n",
    "\n",
    "### 1. All Subsets Feature Selection\n",
    "\n",
    "Examines every possible feature combination to find the best subset.\n",
    "\n",
    "**Procedure:**\n",
    "1. Start with no features (baseline model).\n",
    "2. Add one feature at a time, computing training error for each.\n",
    "3. Select the best one-feature model.\n",
    "4. Move to two-feature models, pick the best.\n",
    "5. Continue until all features are included.\n",
    "\n",
    "**Limitation:**\n",
    "- Computationally infeasible: If there are $ D $ features, the number of models evaluated is $ 2^D $, which becomes impossible for large $ D $.\n",
    "\n",
    "### 2. Greedy Algorithms for Feature Selection\n",
    "\n",
    "To avoid computational cost, we use heuristic approaches:\n",
    "\n",
    "- **Forward Stepwise Selection:**\n",
    "  - Start with no features and add one at a time, choosing the best improvement at each step.\n",
    "  - Limitations: Might miss optimal feature sets because once a feature is included, it stays.\n",
    "\n",
    "- **Backward Stepwise Selection:**\n",
    "  - Start with all features and remove them one by one.\n",
    "\n",
    "- **Hybrid Methods:** Add/remove features dynamically (e.g., stepwise selection with elimination).\n",
    "\n",
    "**Comparison:**\n",
    "\n",
    "| Method           | Pros                        | Cons                                      |\n",
    "|------------------|-----------------------------|-------------------------------------------|\n",
    "| All subsets      | Finds best model            | Computationally infeasible for large $ D $ |\n",
    "| Forward Selection| Faster than all subsets     | Might miss the best subset                |\n",
    "| Backward Selection| Can remove redundant features | Requires fitting full model first         |\n",
    "\n",
    "### 3. Regularization-Based Feature Selection (Lasso Regression)\n",
    "\n",
    "Instead of manually searching for the best subset, Lasso (L1 Regularization) automatically shrinks some feature coefficients to exactly zero, removing them from the model.\n",
    "\n",
    "**Lasso vs. Ridge Regression:**\n",
    "\n",
    "- **Ridge Regression (L2 penalty):**\n",
    "  - Shrinks coefficients but doesn’t set them exactly to zero.\n",
    "  - Works well when all features contribute a little.\n",
    "\n",
    "- **Lasso Regression (L1 penalty):**\n",
    "  - Shrinks some coefficients to exactly zero, performing feature selection.\n",
    "  - Ideal when only a subset of features are relevant.\n",
    "\n",
    "**Why Does Lasso Work?**\n",
    "- Uses L1 norm ($|w|$ instead of $w^2$ in Ridge).\n",
    "- Geometrically, L1 forms a diamond shape constraint, which increases the probability of hitting an axis and setting some coefficients to zero (sparseness).\n",
    "- Unlike thresholding Ridge regression (which fails due to correlated features), Lasso naturally selects features.\n",
    "\n",
    "### 4. Optimization of Lasso: Coordinate Descent\n",
    "\n",
    "Since Lasso lacks a closed-form solution, we solve it using Coordinate Descent, which:\n",
    "\n",
    "- Updates one coefficient at a time while keeping others fixed.\n",
    "- Uses a technique called soft-thresholding, which shrinks small coefficients to zero.\n",
    "- Is computationally efficient and works well in high-dimensional settings.\n",
    "\n",
    "**Algorithm for Lasso (Coordinate Descent):**\n",
    "1. Initialize all weights to zero.\n",
    "2. While not converged:\n",
    "   - Pick a feature $ j $ and compute its effect ($\\rho_j$).\n",
    "   - Update $ w_j $:\n",
    "     - If $\\rho_j$ is small → Set $ w_j = 0 $.\n",
    "     - If $\\rho_j$ is large → Adjust $ w_j $ based on $\\lambda$.\n",
    "\n",
    "**Choosing Lambda ($\\lambda$):**\n",
    "- $\\lambda$ controls sparsity:\n",
    "  - Small $\\lambda$ → Less regularization, more features retained.\n",
    "  - Large $\\lambda$ → More regularization, fewer features retained.\n",
    "- Select optimal $\\lambda$ using cross-validation.\n",
    "\n",
    "# Nonparametric Regression Approaches\n",
    "\n",
    "Moving beyond fixed-feature models to more flexible approaches like k-Nearest Neighbors (k-NN) and Kernel Regression.\n",
    "These methods allow model complexity to grow with data.\n",
    "\n",
    "## Local vs. Global Fits\n",
    "\n",
    "Traditional regression fits a single function over the entire dataset.\n",
    "In contrast, nonparametric approaches allow local adjustments, meaning different regions of the input space can have different behaviors.\n",
    "\n",
    "### K-Nearest Neighbors (k-NN) Regression\n",
    "\n",
    "**Concept:** Instead of fitting a global function, k-NN finds the k most similar points in the dataset and averages their values.\n",
    "- 1-Nearest Neighbor (1-NN): The simplest case, where we predict using the closest data point.\n",
    "\n",
    "**Distance Metric Matters:**\n",
    "- The choice of distance (Euclidean, Manhattan, weighted distances) impacts which neighbors are selected.\n",
    "- A Voronoi diagram can be used to visualize regions where each point is the nearest neighbor.\n",
    "\n",
    "**Higher-Dimensional Spaces:** k-NN generalizes, but high-dimensional data requires careful handling due to sparse observations (Curse of Dimensionality).\n",
    "\n",
    "**Key Trade-offs:**\n",
    "- Too small k (e.g., 1-NN) → High variance, overfits to noise.\n",
    "- Too large k → High bias, oversmooths trends.\n",
    "\n",
    "### Weighted k-NN and Kernel Regression\n",
    "\n",
    "- **Weighted k-NN:** Assigns weights to neighbors based on proximity (closer points contribute more).\n",
    "- **Kernel Regression:** Extends this idea by applying weights to all observations, not just a fixed number of neighbors.\n",
    "\n",
    "**Common Kernel Functions:**\n",
    "- Gaussian Kernel (smoothest)\n",
    "- Epanechnikov Kernel (faster decay)\n",
    "- Boxcar Kernel (hard cutoffs)\n",
    "\n",
    "**Kernel Bandwidth (λ):**\n",
    "- Small λ → High variance (fits details, risk of overfitting).\n",
    "- Large λ → High bias (overly smooth, underfits data).\n",
    "\n",
    "**Choosing λ or k?**\n",
    "- Cross-validation is typically used to tune these hyperparameters.\n",
    "\n",
    "### Locally Weighted Regression\n",
    "\n",
    "Instead of fitting a constant locally (as in kernel regression), we can fit local polynomials (e.g., locally weighted linear regression).\n",
    "- **Key Benefit:** Reduces boundary effects and improves performance in curved regions.\n",
    "\n",
    "## Theoretical and Practical\n",
    "\n",
    "## Theoretical and Practical Aspects\n",
    "\n",
    "- **Nonparametric models scale with data:** More data = better fits.\n",
    "- **Mean Squared Error (MSE) Convergence:**\n",
    "  - If data is noise-free, 1-NN error approaches zero with enough data.\n",
    "  - If data is noisy, k-NN must increase k as data grows to reduce variance.\n",
    "- **Curse of Dimensionality:**\n",
    "  - As dimensions grow, data becomes sparse, making nearest-neighbor search inefficient.\n",
    "  - Dimensionality reduction or careful feature engineering is crucial.\n",
    "\n",
    "### k-NN for Classification\n",
    "\n",
    "Instead of averaging outputs, we assign labels based on majority voting among k neighbors.\n",
    "- **Example:** Spam filtering based on similarity to past labeled emails.\n",
    "- High k smooths decision boundaries, reducing overfitting.\n",
    "\n",
    "\n",
    "# Classification\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "- [Classification](#classification)\n",
    "  - [1. What is a Linear Classifier?](#1-what-is-a-linear-classifier)\n",
    "  - [2. Decision Boundaries in Linear Classifiers](#2-decision-boundaries-in-linear-classifiers)\n",
    "  - [3. Logistic Regression: Moving Beyond Just Labels](#3-logistic-regression-moving-beyond-just-labels)\n",
    "  - [4. Quick Review of Probability Concepts](#4-quick-review-of-probability-concepts)\n",
    "  - [5. Logistic Regression & the Sigmoid Function](#5-logistic-regression--the-sigmoid-function)\n",
    "  - [6. Learning Logistic Regression from Data](#6-learning-logistic-regression-from-data)\n",
    "  - [7. Categorical Features & One-Hot Encoding](#7-categorical-features--one-hot-encoding)\n",
    "  - [8. Multi-Class Classification with One-vs-All](#8-multi-class-classification-with-one-vs-all)\n",
    "  - [9. Quick Recap of Logistic Regression](#9-quick-recap-of-logistic-regression)\n",
    "  - [10. Learning the Parameters ($ w $)](#10-learning-the-parameters-w)\n",
    "  - [11. The Likelihood Function](#11-the-likelihood-function)\n",
    "  - [12. Why Use Log Likelihood Instead?](#12-why-use-log-likelihood-instead)\n",
    "  - [13. Optimization: Gradient Ascent](#13-optimization-gradient-ascent)\n",
    "  - [14. How the Gradient Works](#14-how-the-gradient-works)\n",
    "  - [15. Step Size ($ \\eta $) & Learning Rate Tuning](#15-step-size--learning-rate-tuning)\n",
    "  - [16. Interpretation of Updates](#16-interpretation-of-updates)\n",
    "  - [17. Final Gradient Ascent Algorithm for Logistic Regression](#17-final-gradient-ascent-algorithm-for-logistic-regression)\n",
    "  - [Understanding Overfitting in Classification](#understanding-overfitting-in-classification)\n",
    "  - [Measuring Classification Error](#measuring-classification-error)\n",
    "  - [How Overfitting Looks in Classification](#how-overfitting-looks-in-classification)\n",
    "  - [Overconfidence in Overfit Models](#overconfidence-in-overfit-models)\n",
    "  - [Regularization: Preventing Overfitting](#regularization-preventing-overfitting)\n",
    "  - [How Regularization Improves Generalization](#how-regularization-improves-generalization)\n",
    "  - [Implementing Regularization with Gradient Ascent](#implementing-regularization-with-gradient-ascent)\n",
    "  - [L1 Regularization & Sparsity](#l1-regularization--sparsity)\n",
    "  - [How to Choose the Right Regularization Parameter ($ \\lambda $)](#how-to-choose-the-right-regularization-parameter-)\n",
    "  - [The Reality of Missing Data](#the-reality-of-missing-data)\n",
    "  - [Common Approaches to Handling Missing Data](#common-approaches-to-handling-missing-data)\n",
    "  - [Approach 1: Skipping Missing Data (Purification)](#approach-1-skipping-missing-data-purification)\n",
    "  - [Approach 2: Imputation (Filling in Missing Values)](#approach-2-imputation-filling-in-missing-values)\n",
    "  - [Approach 3: Modifying Decision Trees to Handle Missing Data](#approach-3-modifying-decision-trees-to-handle-missing-data)\n",
    "  - [Making Decision Trees Robust to Missing Data](#making-decision-trees-robust-to-missing-data)\n",
    "  - [Example: Handling a Loan Application with Missing Income](#example-handling-a-loan-application-with-missing-income)\n",
    "  - [Optimizing Decision Trees for Missing Data](#optimizing-decision-trees-for-missing-data)\n",
    "  - [What is Boosting?](#what-is-boosting)\n",
    "  - [The Idea Behind Boosting](#the-idea-behind-boosting)\n",
    "  - [How Boosting Works](#how-boosting-works)\n",
    "  - [Understanding the AdaBoost Algorithm](#understanding-the-adaboost-algorithm)\n",
    "  - [How AdaBoost Improves Accuracy](#how-adaboost-improves-accuracy)\n",
    "  - [Why Does AdaBoost Work?](#why-does-adaboost-work)\n",
    "  - [Overfitting in Boosting](#overfitting-in-boosting)\n",
    "  - [Comparison: AdaBoost vs. Random Forest](#comparison-adaboost-vs-random-forest)\n",
    "  - [Boosting in the Real World](#boosting-in-the-real-world)\n",
    "  - [Why Accuracy is Not Enough?](#why-accuracy-is-not-enough)\n",
    "  - [The Restaurant Review Example](#the-restaurant-review-example)\n",
    "  - [Understanding Precision and Recall](#understanding-precision-and-recall)\n",
    "  - [Precision vs. Recall Trade-off](#precision-vs-recall-trade-off)\n",
    "  - [False Positives vs. False Negatives](#false-positives-vs-false-negatives)\n",
    "  - [Optimizing the Trade-off: Adjusting the Threshold](#optimizing-the-trade-off-adjusting-the-threshold)\n",
    "  - [Precision-Recall Curve](#precision-recall-curve)\n",
    "  - [Precision@K: A Practical Metric](#precisionk-a-practical-metric)\n",
    "  - [The Challenge of Large Datasets](#the-challenge-of-large-datasets)\n",
    "  - [Why Traditional Gradient Descent Fails on Big Data](#why-traditional-gradient-descent-fails-on-big-data)\n",
    "  - [Stochastic Gradient Descent (SGD) – A Game Changer](#stochastic-gradient-descent-sgd--a-game-changer)\n",
    "  - [Comparing Gradient Descent vs. Stochastic Gradient Descent](#comparing-gradient-descent-vs-stochastic-gradient-descent)\n",
    "  - [The Role of Online Learning](#the-role-of-online-learning)\n",
    "  - [Practical Challenges with SGD & Online Learning](#practical-challenges-with-sgd--online-learning)\n",
    "  - [Distributed & Parallel Machine Learning](#distributed--parallel-machine-learning)\n",
    "\n",
    "\n",
    "# Classification\n",
    "\n",
    "Classification is one of the most widely used and fundamental areas of Machine Learning. It focuses on building models that assign discrete labels to inputs.\n",
    "\n",
    "A classifier maps input features (X) to an output class (Y).\n",
    "\n",
    "**Examples:**\n",
    "- Spam filters classify emails as spam or not spam.\n",
    "- Multi-class classification: Categorizing content (e.g., an ad system determining if a webpage is about finance, education, or technology).\n",
    "- Image classification: Predicting the category of an image (e.g., dog breeds in ImageNet).\n",
    "- Medical applications: Personalized medicine can use classification models to predict the best treatment based on DNA and lifestyle.\n",
    "- Mind-reading models: fMRI-based classifiers can predict what a person is thinking by analyzing brain scans.\n",
    "\n",
    "**Why is Classification Important?**\n",
    "- Used everywhere: spam detection, web search ranking, medical diagnosis, and recommendation systems.\n",
    "- Core techniques apply to almost all supervised learning problems.\n",
    "\n",
    "## 1. What is a Linear Classifier?\n",
    "\n",
    "A linear classifier predicts whether an input belongs to one class (+1) or another (-1) by computing a weighted sum of input features.\n",
    "\n",
    "**Example: Sentiment Analysis**\n",
    "- A classifier assigns weights to words in a review.\n",
    "- Positive words (e.g., \"awesome\") have positive weights.\n",
    "- Negative words (e.g., \"awful\") have negative weights.\n",
    "- If the overall score is > 0, it’s classified as positive; otherwise, it's negative.\n",
    "\n",
    "## 2. Decision Boundaries in Linear Classifiers\n",
    "\n",
    "The decision boundary is a hyperplane (a line in 2D, a plane in 3D, and so on) that separates classes.\n",
    "\n",
    "**Example:**\n",
    "- \"Awesome\" has a weight of +1.0, \"Awful\" has a weight of -1.5.\n",
    "- If a sentence contains more \"awesomes\" than \"awfuls,\" it gets classified as positive.\n",
    "- The boundary equation:\n",
    "  $\n",
    "  1.0 \\times (\\# \\text{awesomes}) - 1.5 \\times (\\# \\text{awfuls}) = 0\n",
    "  $\n",
    "- Everything below the line is classified as positive.\n",
    "- Everything above the line is classified as negative.\n",
    "\n",
    "## 3. Logistic Regression: Moving Beyond Just Labels\n",
    "\n",
    "Logistic Regression extends linear classifiers by predicting probabilities instead of hard labels. The output is not just \"positive\" or \"negative\" but a confidence score.\n",
    "\n",
    "**Example:**\n",
    "- “The sushi & everything was awesome!” → 99% probability of positive\n",
    "- “The sushi was good, the service was okay.” → 55% probability of positive\n",
    "\n",
    "This is useful when some classifications are uncertain.\n",
    "\n",
    "## 4. Quick Review of Probability Concepts\n",
    "\n",
    "- Probability is always between 0 and 1.\n",
    "- Sum of all probabilities = 1.\n",
    "- Conditional probability: The probability of an event given some condition.\n",
    "\n",
    "**Example:**\n",
    "$\n",
    "P(\\text{review is positive} \\mid \\text{contains \"awesome\"}) = 0.9\n",
    "$\n",
    "This means that, given a review contains \"awesome,\" 90% of such reviews are positive.\n",
    "\n",
    "## 5. Logistic Regression & the Sigmoid Function\n",
    "\n",
    "How do we convert a score (which ranges from -∞ to +∞) into a probability (0 to 1)? The answer: The Sigmoid Function.\n",
    "\n",
    "**Formula:**\n",
    "$\n",
    "P(y = +1 \\mid x) = \\frac{1}{1 + e^{-w^T x}}\n",
    "$\n",
    "\n",
    "**Properties:**\n",
    "- If score → +∞, probability → 1 (very confident positive).\n",
    "- If score → -∞, probability → 0 (very confident negative).\n",
    "- If score = 0, probability = 0.5 (uncertain).\n",
    "\n",
    "Sigmoid acts as a \"squashing function,\" keeping outputs between 0 and 1.\n",
    "\n",
    "## 6. Learning Logistic Regression from Data\n",
    "\n",
    "**Process:**\n",
    "- Training Data → Feature Extraction → Learn Parameters $ w $ → Predict Sentiment\n",
    "\n",
    "**Likelihood Function:**\n",
    "- Measures how good a set of parameters $ w $ is for classification.\n",
    "- Example: Different decision boundaries will give different likelihood scores.\n",
    "\n",
    "## 7. Categorical Features & One-Hot Encoding\n",
    "\n",
    "Machine learning models handle numeric data better than categorical data (e.g., \"Country\" or \"Gender\").\n",
    "\n",
    "**Solution:** Convert categorical variables into one-hot vectors.\n",
    "\n",
    "**Example:**\n",
    "- Instead of \"Country: Brazil,\" we create binary features:\n",
    "  - Argentina → 0\n",
    "  - Brazil → 1\n",
    "  - Zimbabwe → 0\n",
    "\n",
    "**Bag-of-Words Encoding** is a common technique for text data.\n",
    "\n",
    "**Example:**\n",
    "- Sentence: \"The sushi was amazing, but service was slow.\"\n",
    "- Convert it into:\n",
    "  - $ h_1 = 1 $ (number of “amazing”)\n",
    "  - $ h_2 = 1 $ (number of “slow”)\n",
    "  - $ h_3 = 1 $ (number of “sushi”)\n",
    "\n",
    "## 8. Multi-Class Classification with One-vs-All\n",
    "\n",
    "Logistic regression is binary, but what if we have more than two classes?\n",
    "\n",
    "**One-vs-All Strategy:**\n",
    "- Train one classifier per class, treating it as class vs. everything else.\n",
    "\n",
    "**Example: Triangle, Heart, Donut**\n",
    "- Classifier 1: Is it a triangle vs. not a triangle?\n",
    "- Classifier 2: Is it a heart vs. not a heart?\n",
    "- Classifier 3: Is it a donut vs. not a donut?\n",
    "\n",
    "Choose the class with the highest probability.\n",
    "\n",
    "## 9. Quick Recap of Logistic Regression\n",
    "\n",
    "**Goal:** Learn a classifier that predicts $ P(y \\mid x) $, where $ y $ is the output label (positive or negative sentiment), and $ x $ is the input (e.g., a restaurant review).\n",
    "\n",
    "- Logistic regression assigns weights ($ w $) to input features (e.g., words in a review) and computes a score.\n",
    "- This score is passed through the sigmoid function to squash it between 0 and 1, giving a probability.\n",
    "\n",
    "## 10. Learning the Parameters ($ w $)\n",
    "\n",
    "**Objective:** Find the best weights ($ w $) so that the model accurately classifies inputs.\n",
    "\n",
    "- Training data consists of ($ x, y $) pairs, where $ x $ is the input, and $ y $ is the true label (positive or negative).\n",
    "- We want to maximize the probability of correct classifications across all training examples.\n",
    "\n",
    "## 11. The Likelihood Function\n",
    "\n",
    "The likelihood function quantifies how well a model fits the data.\n",
    "\n",
    "- Higher likelihood = better model fit.\n",
    "- Given a dataset of $ N $ examples:\n",
    "  - For positive examples, we maximize $ P(y = +1 \\mid x, w) $.\n",
    "  - For negative examples, we maximize $ P(y = -1 \\mid x, w) $.\n",
    "- The overall likelihood function is the product of probabilities across all training examples.\n",
    "\n",
    "**Example: Computing Likelihood**\n",
    "- Suppose we have 4 reviews:\n",
    "  - (2 \"awesomes\", 1 \"awful\", positive review) → Want high $ P(y = +1 \\mid x) $\n",
    "  - (0 \"awesomes\", 2 \"awfuls\", negative review) → Want high $ P(y = -1 \\mid x) $\n",
    "\n",
    "**Likelihood function:**\n",
    "$\n",
    "L(w) = P(y_1 \\mid x_1, w) \\times P(y_2 \\mid x_2, w) \\times \\ldots \\times P(y_N \\mid x_N, w)\n",
    "$\n",
    "We seek $ w $ that maximizes this likelihood function.\n",
    "\n",
    "## 12. Why Use Log Likelihood Instead?\n",
    "\n",
    "Since likelihood is a product of many probabilities, it results in very small values. Instead of maximizing $ L(w) $, we maximize its log (log-likelihood function), which turns the product into a sum:\n",
    "\n",
    "$\n",
    "\\log L(w) = \\sum_{i=1}^{N} \\log P(y_i \\mid x_i, w)\n",
    "$\n",
    "\n",
    "Taking the log simplifies calculations and makes optimization easier.\n",
    "\n",
    "## 13. Optimization: Gradient Ascent\n",
    "\n",
    "Gradient ascent is used to find the optimal weights $ w $ that maximize the log-likelihood function.\n",
    "\n",
    "**Key idea:** Start with random weights and iteratively adjust them in the direction of increasing likelihood.\n",
    "\n",
    "**Gradient Ascent Algorithm:**\n",
    "1. Initialize $ w $ randomly.\n",
    "2. Compute the gradient (how much each weight should change).\n",
    "3. Update each weight using:\n",
    "   $\n",
    "   w_j = w_j + \\eta \\cdot \\frac{\\partial \\log L}{\\partial w_j}\n",
    "   $\n",
    "   - $ \\eta $ (step size) controls how big the update is.\n",
    "4. Stop when updates become small (converged).\n",
    "\n",
    "## 14. How the Gradient Works\n",
    "\n",
    "The gradient measures the direction and magnitude of change needed for each weight.\n",
    "\n",
    "**Formula for updating $ w_j $:**\n",
    "$\n",
    "\\frac{\\partial \\log L}{\\partial w_j} = \\sum_{i=1}^{N} (y_i - P(y_i = +1 \\mid x_i, w)) \\cdot x_{ij}\n",
    "$\n",
    "\n",
    "**Interpretation:**\n",
    "- If a positive review is misclassified as negative, increase weights for positive words.\n",
    "- If a negative review is misclassified as positive, decrease weights for negative words.\n",
    "- If a review is classified correctly, don’t change much.\n",
    "\n",
    "## 15. Step Size ($ \\eta $) & Learning Rate Tuning\n",
    "\n",
    "Choosing the right step size ($ \\eta $) is crucial.\n",
    "\n",
    "- If $ \\eta $ is too small: The model learns slowly, taking too long to converge.\n",
    "- If $ \\eta $ is too large: The model oscillates or diverges, failing to find the optimal weights.\n",
    "\n",
    "**How to Find the Best Step Size?**\n",
    "- Learning curves: Track log-likelihood over iterations.\n",
    "  - Too small $ \\eta $ → Converges slowly.\n",
    "  - Too large $ \\eta $ → Wild oscillations.\n",
    "  - Optimal $ \\eta $ → Smooth increase in log-likelihood, reaching a maximum efficiently.\n",
    "\n",
    "## 16. Interpretation of Updates\n",
    "\n",
    "- If a training example is classified correctly, little to no change in weights.\n",
    "- If an example is misclassified:\n",
    "  - If it should be positive, increase its word weights.\n",
    "  - If it should be negative, decrease its word weights.\n",
    "\n",
    "**Example Calculation:**\n",
    "- Suppose a review has 2 “awesomes”, 1 “awful”, and is positive.\n",
    "- If the current model predicts 0.5 probability, the update is:\n",
    "  $\n",
    "  w_j = w_j + \\eta \\cdot (1 - 0.5) \\cdot 2\n",
    "  $\n",
    "  Increase $ w_j $ since the model was uncertain about a positive review.\n",
    "\n",
    "## 17. Final Gradient Ascent Algorithm for Logistic Regression\n",
    "\n",
    "1. Initialize weights $ w $ randomly.\n",
    "2. Repeat until convergence:\n",
    "   - For each weight $ w_j $, update using:\n",
    "     $\n",
    "     w_j = w_j + \\eta \\sum_{i=1}^{N} (y_i - P(y_i = +1 \\mid x_i, w)) \\cdot x_{ij}\n",
    "     $\n",
    "3. Stop when updates are very small.\n",
    "\n",
    "**Note:** The algorithm iteratively adjusts weights to maximize the log-likelihood function, converging to the best weights for the model.\n",
    "\n",
    "## Understanding Overfitting in Classification\n",
    "\n",
    "Overfitting happens when a model performs well on training data but fails to generalize to new data. In classification, overfitting can cause:\n",
    "- Overly complex decision boundaries.\n",
    "- Extremely high confidence in wrong predictions.\n",
    "\n",
    "**Example:** A classifier that perfectly fits training data might memorize noise rather than learn general patterns.\n",
    "\n",
    "## Measuring Classification Error\n",
    "\n",
    "We evaluate classifiers using classification error:\n",
    "$ \\text{Error} = \\frac{\\text{Number of misclassified examples}}{\\text{Total number of examples}} $\n",
    "\n",
    "Accuracy = 1 - Error. Classifiers should be evaluated on a separate validation set to detect overfitting.\n",
    "\n",
    "## How Overfitting Looks in Classification\n",
    "\n",
    "Overfitting can happen in logistic regression when decision boundaries become too complex.\n",
    "\n",
    "**Example:**\n",
    "- A simple linear classifier correctly separates most data.\n",
    "- A quadratic classifier captures more nuances, improving accuracy.\n",
    "- A high-degree polynomial (e.g., degree 20) creates wildly complex decision boundaries.\n",
    "\n",
    "**Problem:** Complex boundaries don’t generalize and may misclassify new data.\n",
    "\n",
    "**Key Signs of Overfitting:**\n",
    "- Decision boundaries become highly irregular.\n",
    "- Large coefficient values (weights become extreme).\n",
    "- Extreme confidence (probabilities near 0 or 1) for uncertain cases.\n",
    "\n",
    "## Overconfidence in Overfit Models\n",
    "\n",
    "Logistic regression models output probabilities, but overfit models push probabilities toward 0 or 1.\n",
    "\n",
    "**Example:** A review with 2 \"awesomes\" and 1 \"awful\" should have a reasonable probability (e.g., 73%) of being positive. If coefficients become too large, the model wrongly becomes 99.7% sure it's positive.\n",
    "\n",
    "**Problem:** The model loses uncertainty and becomes overconfident in wrong predictions.\n",
    "\n",
    "## Regularization: Preventing Overfitting\n",
    "\n",
    "Solution: Regularization penalizes large weights, making the model simpler and more generalizable.\n",
    "\n",
    "**Two types of regularization:**\n",
    "- **L2 Regularization (Ridge Regression in Regression):** Penalizes large weights using sum of squared coefficients:\n",
    "  $ \\lambda \\sum w_j^2 $\n",
    "  Helps reduce overfitting while maintaining smooth decision boundaries.\n",
    "  **Effects:**\n",
    "  - Keeps coefficients small but nonzero.\n",
    "  - Prevents extreme decision boundaries.\n",
    "  - Balances training fit vs. generalization.\n",
    "\n",
    "- **L1 Regularization (Lasso in Regression):** Penalizes large weights using sum of absolute values:\n",
    "  $ \\lambda \\sum |w_j| $\n",
    "  Encourages sparsity → many weights become exactly 0.\n",
    "  Useful when working with high-dimensional data (e.g., spam detection with thousands of features).\n",
    "  **Effects:**\n",
    "  - Fewer active features (improves interpretability & efficiency).\n",
    "  - Sparse solutions → only important features remain.\n",
    "\n",
    "## How Regularization Improves Generalization\n",
    "\n",
    "**Example:** Applying L2 Regularization on a Degree-20 Model\n",
    "- Without regularization → crazy decision boundary, large coefficients (3000+).\n",
    "- With L2 regularization → smooth, well-behaved boundary.\n",
    "\n",
    "**Effect on Probabilities:**\n",
    "- Without regularization → Overconfident probabilities (near 0 or 1).\n",
    "- With regularization → Reasonable confidence levels (proper uncertainty maintained).\n",
    "\n",
    "## Implementing Regularization with Gradient Ascent\n",
    "\n",
    "Gradient Ascent for Regularized Logistic Regression:\n",
    "The update rule adds a penalty term:\n",
    "$ w_j = w_j + \\eta \\left( \\sum_{i=1}^{N} (y_i - P(y_i = +1 \\mid x_i, w)) x_{ij} - 2\\lambda w_j \\right) $\n",
    "\n",
    "**Effect:**\n",
    "- Reduces large coefficients over time.\n",
    "- Helps maintain smooth, generalizable decision boundaries.\n",
    "\n",
    "**Implementation Change:** Only one small change! Add $-2\\lambda w_j$ to your existing gradient ascent code.\n",
    "\n",
    "## L1 Regularization & Sparsity\n",
    "\n",
    "L1 Regularization shrinks some coefficients to exactly zero.\n",
    "\n",
    "**Why is this useful?**\n",
    "- Reduces computation (faster predictions).\n",
    "- Improves interpretability (removes unnecessary features).\n",
    "\n",
    "**Example: Word Importance in Sentiment Analysis**\n",
    "- L2 Regularization: Words like “great”, “bad”, and “disappointed” get small weights.\n",
    "- L1 Regularization: Some words get completely removed (zero weight), leaving only the most important.\n",
    "\n",
    "## How to Choose the Right Regularization Parameter (λ)?\n",
    "\n",
    "- λ too small → Overfitting still happens.\n",
    "- λ too large → Model oversimplifies (underfitting).\n",
    "\n",
    "**Best approach:** Use cross-validation or a validation set to tune λ.\n",
    "\n",
    "## The Reality of Missing Data\n",
    "\n",
    "In previous modules, we assumed that every data point had all feature values.\n",
    "In reality, datasets are often incomplete:\n",
    "- Loan applications may be missing income or credit history.\n",
    "- Medical records may lack certain test results.\n",
    "- Customer data may have unknown age, address, etc.\n",
    "\n",
    "Missing data can impact ML models at:\n",
    "- Training time → We can’t properly train if features are missing.\n",
    "- Prediction time → The model doesn’t know what to do with missing inputs.\n",
    "\n",
    "## Common Approaches to Handling Missing Data\n",
    "\n",
    "We discuss three main strategies:\n",
    "\n",
    "| Approach | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| Skipping Missing Data | Simple, easy to implement | Reduces dataset size, risks removing valuable information |\n",
    "| Imputation (Filling Missing Values) | Preserves data | Introduces bias, makes incorrect assumptions |\n",
    "| Modifying the Model to Handle Missing Data | More accurate, adapts to missing values | Requires modifying algorithms |\n",
    "\n",
    "## Approach 1: Skipping Missing Data (Purification)\n",
    "\n",
    "The simplest solution: Remove rows or features with missing values.\n",
    "\n",
    "**Two options:**\n",
    "1. Skip rows with missing values (Reduces dataset size).\n",
    "2. Skip entire features if too many values are missing.\n",
    "\n",
    "**Example**\n",
    "| Credit | Term | Income | Risky Loan? |\n",
    "|--------|------|--------|-------------|\n",
    "| Poor | 3Y | High | Yes |\n",
    "| Fair | ? | High | No |\n",
    "| Excellent | 5Y | ? | No |\n",
    "\n",
    "- If only a few rows have missing values, dropping them is fine.\n",
    "- If many values are missing, removing them may destroy useful data.\n",
    "\n",
    "**Problems with Skipping Data**\n",
    "- ❌ If many rows are removed, we lose valuable information.\n",
    "- ❌ If many features are dropped, we may miss important patterns.\n",
    "- ❌ Doesn’t solve the issue at prediction time (what if a missing value appears in new data?).\n",
    "\n",
    "✅ Good when missing values are rare, but not a scalable solution.\n",
    "\n",
    "## Approach 2: Imputation (Filling in Missing Values)\n",
    "\n",
    "Instead of removing missing data, we fill it in using estimated values.\n",
    "\n",
    "**Example:** If 70% of loans are 3-year loans, replace missing “Term” values with 3 years.\n",
    "\n",
    "**Common Imputation Strategies:**\n",
    "- For Categorical Features (e.g., Credit Score: Excellent/Fair/Poor)\n",
    "  - Replace missing values with the most common category.\n",
    "- For Numerical Features (e.g., Income)\n",
    "  - Replace missing values with the mean/median of the observed values.\n",
    "\n",
    "**Problems with Simple Imputation**\n",
    "- ❌ Can introduce bias (e.g., assuming everyone missing \"age\" is 40).\n",
    "- ❌ Can misrepresent reality (e.g., assuming all missing loans are 3-year loans).\n",
    "- ❌ May not be correct at prediction time.\n",
    "\n",
    "✅ Better than skipping data, but introduces systematic errors.\n",
    "\n",
    "## Approach 3: Modifying Decision Trees to Handle Missing Data\n",
    "\n",
    "Instead of skipping or guessing, modify decision trees to handle missing values natively.\n",
    "\n",
    "**How Does This Work?**\n",
    "- Allow the model to learn how to deal with missing values.\n",
    "- Modify decision trees to assign missing values to a specific branch.\n",
    "- **Example:** If Income is missing, send it down the \"Low Income\" branch.\n",
    "- This optimizes tree splits to minimize classification error.\n",
    "\n",
    "## Making Decision Trees Robust to Missing Data\n",
    "\n",
    "Normally, decision trees split on features (e.g., Credit Score).\n",
    "But if Credit Score is missing, what should we do?\n",
    "\n",
    "**Solution:** Assign missing values to the best branch based on past# Classification\n",
    "\n",
    "Classification is one of the most widely used and fundamental areas of Machine Learning. It focuses on building models that assign discrete labels to inputs.\n",
    "\n",
    "A classifier maps input features (X) to an output class (Y).\n",
    "\n",
    "**Examples:**\n",
    "- Spam filters classify emails as spam or not spam.\n",
    "- Multi-class classification: Categorizing content (e.g., an ad system determining if a webpage is about finance, education, or technology).\n",
    "- Image classification: Predicting the category of an image (e.g., dog breeds in ImageNet).\n",
    "- Medical applications: Personalized medicine can use classification models to predict the best treatment based on DNA and lifestyle.\n",
    "- Mind-reading models: fMRI-based classifiers can predict what a person is thinking by analyzing brain scans.\n",
    "\n",
    "**Why is Classification Important?**\n",
    "- Used everywhere: spam detection, web search ranking, medical diagnosis, and recommendation systems.\n",
    "- Core techniques apply to almost all supervised learning problems.\n",
    "\n",
    "## 1. What is a Linear Classifier?\n",
    "\n",
    "A linear classifier predicts whether an input belongs to one class (+1) or another (-1) by computing a weighted sum of input features.\n",
    "\n",
    "**Example: Sentiment Analysis**\n",
    "- A classifier assigns weights to words in a review.\n",
    "- Positive words (e.g., \"awesome\") have positive weights.\n",
    "- Negative words (e.g., \"awful\") have negative weights.\n",
    "- If the overall score is > 0, it’s classified as positive; otherwise, it's negative.\n",
    "\n",
    "## 2. Decision Boundaries in Linear Classifiers\n",
    "\n",
    "The decision boundary is a hyperplane (a line in 2D, a plane in 3D, and so on) that separates classes.\n",
    "\n",
    "**Example:**\n",
    "- \"Awesome\" has a weight of +1.0, \"Awful\" has a weight of -1.5.\n",
    "- If a sentence contains more \"awesomes\" than \"awfuls,\" it gets classified as positive.\n",
    "- The boundary equation:\n",
    "  $\n",
    "  1.0 \\times (\\# \\text{awesomes}) - 1.5 \\times (\\# \\text{awfuls}) = 0\n",
    "  $\n",
    "- Everything below the line is classified as positive.\n",
    "- Everything above the line is classified as negative.\n",
    "\n",
    "## 3. Logistic Regression: Moving Beyond Just Labels\n",
    "\n",
    "Logistic Regression extends linear classifiers by predicting probabilities instead of hard labels. The output is not just \"positive\" or \"negative\" but a confidence score.\n",
    "\n",
    "**Example:**\n",
    "- “The sushi & everything was awesome!” → 99% probability of positive\n",
    "- “The sushi was good, the service was okay.” → 55% probability of positive\n",
    "\n",
    "This is useful when some classifications are uncertain.\n",
    "\n",
    "## 4. Quick Review of Probability Concepts\n",
    "\n",
    "- Probability is always between 0 and 1.\n",
    "- Sum of all probabilities = 1.\n",
    "- Conditional probability: The probability of an event given some condition.\n",
    "\n",
    "**Example:**\n",
    "$\n",
    "P(\\text{review is positive} \\mid \\text{contains \"awesome\"}) = 0.9\n",
    "$\n",
    "This means that, given a review contains \"awesome,\" 90% of such reviews are positive.\n",
    "\n",
    "## 5. Logistic Regression & the Sigmoid Function\n",
    "\n",
    "How do we convert a score (which ranges from -∞ to +∞) into a probability (0 to 1)? The answer: The Sigmoid Function.\n",
    "\n",
    "**Formula:**\n",
    "$\n",
    "P(y = +1 \\mid x) = \\frac{1}{1 + e^{-w^T x}}\n",
    "$\n",
    "\n",
    "**Properties:**\n",
    "- If score → +∞, probability → 1 (very confident positive).\n",
    "- If score → -∞, probability → 0 (very confident negative).\n",
    "- If score = 0, probability = 0.5 (uncertain).\n",
    "\n",
    "Sigmoid acts as a \"squashing function,\" keeping outputs between 0 and 1.\n",
    "\n",
    "## 6. Learning Logistic Regression from Data\n",
    "\n",
    "**Process:**\n",
    "- Training Data → Feature Extraction → Learn Parameters $ w $ → Predict Sentiment\n",
    "\n",
    "**Likelihood Function:**\n",
    "- Measures how good a set of parameters $ w $ is for classification.\n",
    "- Example: Different decision boundaries will give different likelihood scores.\n",
    "\n",
    "## 7. Categorical Features & One-Hot Encoding\n",
    "\n",
    "Machine learning models handle numeric data better than categorical data (e.g., \"Country\" or \"Gender\").\n",
    "\n",
    "**Solution:** Convert categorical variables into one-hot vectors.\n",
    "\n",
    "**Example:**\n",
    "- Instead of \"Country: Brazil,\" we create binary features:\n",
    "  - Argentina → 0\n",
    "  - Brazil → 1\n",
    "  - Zimbabwe → 0\n",
    "\n",
    "**Bag-of-Words Encoding** is a common technique for text data.\n",
    "\n",
    "**Example:**\n",
    "- Sentence: \"The sushi was amazing, but service was slow.\"\n",
    "- Convert it into:\n",
    "  - $ h_1 = 1 $ (number of “amazing”)\n",
    "  - $ h_2 = 1 $ (number of “slow”)\n",
    "  - $ h_3 = 1 $ (number of “sushi”)\n",
    "\n",
    "## 8. Multi-Class Classification with One-vs-All\n",
    "\n",
    "Logistic regression is binary, but what if we have more than two classes?\n",
    "\n",
    "**One-vs-All Strategy:**\n",
    "- Train one classifier per class, treating it as class vs. everything else.\n",
    "\n",
    "**Example: Triangle, Heart, Donut**\n",
    "- Classifier 1: Is it a triangle vs. not a triangle?\n",
    "- Classifier 2: Is it a heart vs. not a heart?\n",
    "- Classifier 3: Is it a donut vs. not a donut?\n",
    "\n",
    "Choose the class with the highest probability.\n",
    "\n",
    "## 9. Quick Recap of Logistic Regression\n",
    "\n",
    "**Goal:** Learn a classifier that predicts $ P(y \\mid x) $, where $ y $ is the output label (positive or negative sentiment), and $ x $ is the input (e.g., a restaurant review).\n",
    "\n",
    "- Logistic regression assigns weights ($ w $) to input features (e.g., words in a review) and computes a score.\n",
    "- This score is passed through the sigmoid function to squash it between 0 and 1, giving a probability.\n",
    "\n",
    "## 10. Learning the Parameters ($ w $)\n",
    "\n",
    "**Objective:** Find the best weights ($ w $) so that the model accurately classifies inputs.\n",
    "\n",
    "- Training data consists of ($ x, y $) pairs, where $ x $ is the input, and $ y $ is the true label (positive or negative).\n",
    "- We want to maximize the probability of correct classifications across all training examples.\n",
    "\n",
    "## 11. The Likelihood Function\n",
    "\n",
    "The likelihood function quantifies how well a model fits the data.\n",
    "\n",
    "- Higher likelihood = better model fit.\n",
    "- Given a dataset of $ N $ examples:\n",
    "  - For positive examples, we maximize $ P(y = +1 \\mid x, w) $.\n",
    "  - For negative examples, we maximize $ P(y = -1 \\mid x, w) $.\n",
    "- The overall likelihood function is the product of probabilities across all training examples.\n",
    "\n",
    "**Example: Computing Likelihood**\n",
    "- Suppose we have 4 reviews:\n",
    "  - (2 \"awesomes\", 1 \"awful\", positive review) → Want high $ P(y = +1 \\mid x) $\n",
    "  - (0 \"awesomes\", 2 \"awfuls\", negative review) → Want high $ P(y = -1 \\mid x) $\n",
    "\n",
    "**Likelihood function:**\n",
    "$\n",
    "L(w) = P(y_1 \\mid x_1, w) \\times P(y_2 \\mid x_2, w) \\times \\ldots \\times P(y_N \\mid x_N, w)\n",
    "$\n",
    "We seek $ w $ that maximizes this likelihood function.\n",
    "\n",
    "## 12. Why Use Log Likelihood Instead?\n",
    "\n",
    "Since likelihood is a product of many probabilities, it results in very small values. Instead of maximizing $ L(w) $, we maximize its log (log-likelihood function), which turns the product into a sum:\n",
    "\n",
    "$\n",
    "\\log L(w) = \\sum_{i=1}^{N} \\log P(y_i \\mid x_i, w)\n",
    "$\n",
    "\n",
    "Taking the log simplifies calculations and makes optimization easier.\n",
    "\n",
    "## 13. Optimization: Gradient Ascent\n",
    "\n",
    "Gradient ascent is used to find the optimal weights $ w $ that maximize the log-likelihood function.\n",
    "\n",
    "**Key idea:** Start with random weights and iteratively adjust them in the direction of increasing likelihood.\n",
    "\n",
    "**Gradient Ascent Algorithm:**\n",
    "1. Initialize $ w $ randomly.\n",
    "2. Compute the gradient (how much each weight should change).\n",
    "3. Update each weight using:\n",
    "   $\n",
    "   w_j = w_j + \\eta \\cdot \\frac{\\partial \\log L}{\\partial w_j}\n",
    "   $\n",
    "   - $ \\eta $ (step size) controls how big the update is.\n",
    "4. Stop when updates become small (converged).\n",
    "\n",
    "## 14. How the Gradient Works\n",
    "\n",
    "The gradient measures the direction and magnitude of change needed for each weight.\n",
    "\n",
    "**Formula for updating $ w_j $:**\n",
    "$\n",
    "\\frac{\\partial \\log L}{\\partial w_j} = \\sum_{i=1}^{N} (y_i - P(y_i = +1 \\mid x_i, w)) \\cdot x_{ij}\n",
    "$\n",
    "\n",
    "**Interpretation:**\n",
    "- If a positive review is misclassified as negative, increase weights for positive words.\n",
    "- If a negative review is misclassified as positive, decrease weights for negative words.\n",
    "- If a review is classified correctly, don’t change much.\n",
    "\n",
    "## 15. Step Size ($ \\eta $) & Learning Rate Tuning\n",
    "\n",
    "Choosing the right step size ($ \\eta $) is crucial.\n",
    "\n",
    "- If $ \\eta $ is too small: The model learns slowly, taking too long to converge.\n",
    "- If $ \\eta $ is too large: The model oscillates or diverges, failing to find the optimal weights.\n",
    "\n",
    "**How to Find the Best Step Size?**\n",
    "- Learning curves: Track log-likelihood over iterations.\n",
    "  - Too small $ \\eta $ → Converges slowly.\n",
    "  - Too large $ \\eta $ → Wild oscillations.\n",
    "  - Optimal $ \\eta $ → Smooth increase in log-likelihood, reaching a maximum efficiently.\n",
    "\n",
    "## 16. Interpretation of Updates\n",
    "\n",
    "- If a training example is classified correctly, little to no change in weights.\n",
    "- If an example is misclassified:\n",
    "  - If it should be positive, increase its word weights.\n",
    "  - If it should be negative, decrease its word weights.\n",
    "\n",
    "**Example Calculation:**\n",
    "- Suppose a review has 2 “awesomes”, 1 “awful”, and is positive.\n",
    "- If the current model predicts 0.5 probability, the update is:\n",
    "  $\n",
    "  w_j = w_j + \\eta \\cdot (1 - 0.5) \\cdot 2\n",
    "  $\n",
    "  Increase $ w_j $ since the model was uncertain about a positive review.\n",
    "\n",
    "## 17. Final Gradient Ascent Algorithm for Logistic Regression\n",
    "\n",
    "1. Initialize weights $ w $ randomly.\n",
    "2. Repeat until convergence:\n",
    "   - For each weight $ w_j $, update using:\n",
    "     $\n",
    "     w_j = w_j + \\eta \\sum_{i=1}^{N} (y_i - P(y_i = +1 \\mid x_i, w)) \\cdot x_{ij}\n",
    "     $\n",
    "3. Stop when updates are very small.\n",
    "\n",
    "**Note:** The algorithm iteratively adjusts weights to maximize the log-likelihood function, converging to the best weights for the model.\n",
    "\n",
    "## Understanding Overfitting in Classification\n",
    "\n",
    "Overfitting happens when a model performs well on training data but fails to generalize to new data. In classification, overfitting can cause:\n",
    "- Overly complex decision boundaries.\n",
    "- Extremely high confidence in wrong predictions.\n",
    "\n",
    "**Example:** A classifier that perfectly fits training data might memorize noise rather than learn general patterns.\n",
    "\n",
    "## Measuring Classification Error\n",
    "\n",
    "We evaluate classifiers using classification error:\n",
    "$ \\text{Error} = \\frac{\\text{Number of misclassified examples}}{\\text{Total number of examples}} $\n",
    "\n",
    "Accuracy = 1 - Error. Classifiers should be evaluated on a separate validation set to detect overfitting.\n",
    "\n",
    "## How Overfitting Looks in Classification\n",
    "\n",
    "Overfitting can happen in logistic regression when decision boundaries become too complex.\n",
    "\n",
    "**Example:**\n",
    "- A simple linear classifier correctly separates most data.\n",
    "- A quadratic classifier captures more nuances, improving accuracy.\n",
    "- A high-degree polynomial (e.g., degree 20) creates wildly complex decision boundaries.\n",
    "\n",
    "**Problem:** Complex boundaries don’t generalize and may misclassify new data.\n",
    "\n",
    "**Key Signs of Overfitting:**\n",
    "- Decision boundaries become highly irregular.\n",
    "- Large coefficient values (weights become extreme).\n",
    "- Extreme confidence (probabilities near 0 or 1) for uncertain cases.\n",
    "\n",
    "## Overconfidence in Overfit Models\n",
    "\n",
    "Logistic regression models output probabilities, but overfit models push probabilities toward 0 or 1.\n",
    "\n",
    "**Example:** A review with 2 \"awesomes\" and 1 \"awful\" should have a reasonable probability (e.g., 73%) of being positive. If coefficients become too large, the model wrongly becomes 99.7% sure it's positive.\n",
    "\n",
    "**Problem:** The model loses uncertainty and becomes overconfident in wrong predictions.\n",
    "\n",
    "## Regularization: Preventing Overfitting\n",
    "\n",
    "Solution: Regularization penalizes large weights, making the model simpler and more generalizable.\n",
    "\n",
    "**Two types of regularization:**\n",
    "- **L2 Regularization (Ridge Regression in Regression):** Penalizes large weights using sum of squared coefficients:\n",
    "  $ \\lambda \\sum w_j^2 $\n",
    "  Helps reduce overfitting while maintaining smooth decision boundaries.\n",
    "  **Effects:**\n",
    "  - Keeps coefficients small but nonzero.\n",
    "  - Prevents extreme decision boundaries.\n",
    "  - Balances training fit vs. generalization.\n",
    "\n",
    "- **L1 Regularization (Lasso in Regression):** Penalizes large weights using sum of absolute values:\n",
    "  $ \\lambda \\sum |w_j| $\n",
    "  Encourages sparsity → many weights become exactly 0.\n",
    "  Useful when working with high-dimensional data (e.g., spam detection with thousands of features).\n",
    "  **Effects:**\n",
    "  - Fewer active features (improves interpretability & efficiency).\n",
    "  - Sparse solutions → only important features remain.\n",
    "\n",
    "## How Regularization Improves Generalization\n",
    "\n",
    "**Example:** Applying L2 Regularization on a Degree-20 Model\n",
    "- Without regularization → crazy decision boundary, large coefficients (3000+).\n",
    "- With L2 regularization → smooth, well-behaved boundary.\n",
    "\n",
    "**Effect on Probabilities:**\n",
    "- Without regularization → Overconfident probabilities (near 0 or 1).\n",
    "- With regularization → Reasonable confidence levels (proper uncertainty maintained).\n",
    "\n",
    "## Implementing Regularization with Gradient Ascent\n",
    "\n",
    "Gradient Ascent for Regularized Logistic Regression:\n",
    "The update rule adds a penalty term:\n",
    "$ w_j = w_j + \\eta \\left( \\sum_{i=1}^{N} (y_i - P(y_i = +1 \\mid x_i, w)) x_{ij} - 2\\lambda w_j \\right) $\n",
    "\n",
    "**Effect:**\n",
    "- Reduces large coefficients over time.\n",
    "- Helps maintain smooth, generalizable decision boundaries.\n",
    "\n",
    "**Implementation Change:** Only one small change! Add $-2\\lambda w_j$ to your existing gradient ascent code.\n",
    "\n",
    "## L1 Regularization & Sparsity\n",
    "\n",
    "L1 Regularization shrinks some coefficients to exactly zero.\n",
    "\n",
    "**Why is this useful?**\n",
    "- Reduces computation (faster predictions).\n",
    "- Improves interpretability (removes unnecessary features).\n",
    "\n",
    "**Example: Word Importance in Sentiment Analysis**\n",
    "- L2 Regularization: Words like “great”, “bad”, and “disappointed” get small weights.\n",
    "- L1 Regularization: Some words get completely removed (zero weight), leaving only the most important.\n",
    "\n",
    "## How to Choose the Right Regularization Parameter (λ)?\n",
    "\n",
    "- λ too small → Overfitting still happens.\n",
    "- λ too large → Model oversimplifies (underfitting).\n",
    "\n",
    "**Best approach:** Use cross-validation or a validation set to tune λ.\n",
    "\n",
    "## The Reality of Missing Data\n",
    "\n",
    "In previous modules, we assumed that every data point had all feature values.\n",
    "In reality, datasets are often incomplete:\n",
    "- Loan applications may be missing income or credit history.\n",
    "- Medical records may lack certain test results.\n",
    "- Customer data may have unknown age, address, etc.\n",
    "\n",
    "Missing data can impact ML models at:\n",
    "- Training time → We can’t properly train if features are missing.\n",
    "- Prediction time → The model doesn’t know what to do with missing inputs.\n",
    "\n",
    "## Common Approaches to Handling Missing Data\n",
    "\n",
    "We discuss three main strategies:\n",
    "\n",
    "| Approach | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| Skipping Missing Data | Simple, easy to implement | Reduces dataset size, risks removing valuable information |\n",
    "| Imputation (Filling Missing Values) | Preserves data | Introduces bias, makes incorrect assumptions |\n",
    "| Modifying the Model to Handle Missing Data | More accurate, adapts to missing values | Requires modifying algorithms |\n",
    "\n",
    "## Approach 1: Skipping Missing Data (Purification)\n",
    "\n",
    "The simplest solution: Remove rows or features with missing values.\n",
    "\n",
    "**Two options:**\n",
    "1. Skip rows with missing values (Reduces dataset size).\n",
    "2. Skip entire features if too many values are missing.\n",
    "\n",
    "**Example**\n",
    "| Credit | Term | Income | Risky Loan? |\n",
    "|--------|------|--------|-------------|\n",
    "| Poor | 3Y | High | Yes |\n",
    "| Fair | ? | High | No |\n",
    "| Excellent | 5Y | ? | No |\n",
    "\n",
    "- If only a few rows have missing values, dropping them is fine.\n",
    "- If many values are missing, removing them may destroy useful data.\n",
    "\n",
    "**Problems with Skipping Data**\n",
    "- ❌ If many rows are removed, we lose valuable information.\n",
    "- ❌ If many features are dropped, we may miss important patterns.\n",
    "- ❌ Doesn’t solve the issue at prediction time (what if a missing value appears in new data?).\n",
    "\n",
    "✅ Good when missing values are rare, but not a scalable solution.\n",
    "\n",
    "## Approach 2: Imputation (Filling in Missing Values)\n",
    "\n",
    "Instead of removing missing data, we fill it in using estimated values.\n",
    "\n",
    "**Example:** If 70% of loans are 3-year loans, replace missing “Term” values with 3 years.\n",
    "\n",
    "**Common Imputation Strategies:**\n",
    "- For Categorical Features (e.g., Credit Score: Excellent/Fair/Poor)\n",
    "  - Replace missing values with the most common category.\n",
    "- For Numerical Features (e.g., Income)\n",
    "  - Replace missing values with the mean/median of the observed values.\n",
    "\n",
    "**Problems with Simple Imputation**\n",
    "- ❌ Can introduce bias (e.g., assuming everyone missing \"age\" is 40).\n",
    "- ❌ Can misrepresent reality (e.g., assuming all missing loans are 3-year loans).\n",
    "- ❌ May not be correct at prediction time.\n",
    "\n",
    "✅ Better than skipping data, but introduces systematic errors.\n",
    "\n",
    "## Approach 3: Modifying Decision Trees to Handle Missing Data\n",
    "\n",
    "Instead of skipping or guessing, modify decision trees to handle missing values natively.\n",
    "\n",
    "**How Does This Work?**\n",
    "- Allow the model to learn how to deal with missing values.\n",
    "- Modify decision trees to assign missing values to a specific branch.\n",
    "- **Example:** If Income is missing, send it down the \"Low Income\" branch.\n",
    "- This optimizes tree splits to minimize classification error.\n",
    "\n",
    "## Making Decision Trees Robust to Missing Data\n",
    "\n",
    "Normally, decision trees split on features (e.g., Credit Score).\n",
    "But if Credit Score is missing, what should we do?\n",
    "\n",
    "**Solution:** Assign missing values to the best branch based on past\n",
    "\n",
    "\n",
    "## Example: Handling a Loan Application with Missing Income\n",
    "\n",
    "| Credit | Term | Income | Risky Loan? |\n",
    "|--------|------|--------|-------------|\n",
    "| Poor   | 3Y   | ?      | Yes         |\n",
    "\n",
    "### Regular Decision Tree\n",
    "\n",
    "- Split on Credit Score → Poor.\n",
    "- Split on Income → Missing Value → ❌ Can’t Proceed.\n",
    "\n",
    "### Modified Decision Tree\n",
    "\n",
    "- Split on Credit Score → Poor.\n",
    "- Income is Missing → Send to “Low Income” branch (or another branch based on error minimization).\n",
    "- Prediction is made despite missing data.\n",
    "\n",
    "## Optimizing Decision Trees for Missing Data\n",
    "\n",
    "Every decision node in the tree decides where to send missing values. The algorithm learns from data where missing values should go.\n",
    "\n",
    "### Key Idea\n",
    "\n",
    "Choose the branch that minimizes classification error.\n",
    "\n",
    "### Example: Choosing the Best Split for Missing Data\n",
    "\n",
    "1. Try placing missing values in Branch A.\n",
    "2. Try placing missing values in Branch B.\n",
    "3. Choose the branch that results in lower classification error.\n",
    "\n",
    "### Advantages of This Approach\n",
    "\n",
    "- Works both at training and prediction time.\n",
    "- Doesn’t remove data (no lost information).\n",
    "- More accurate than imputation.\n",
    "- Automatically handles missing data in a meaningful way.\n",
    "\n",
    "## What is Boosting?\n",
    "\n",
    "Boosting is a meta-learning technique that enhances the performance of weak classifiers. It iteratively trains weak models, giving more weight to the misclassified examples in each round.\n",
    "\n",
    "**Common weak classifiers used in boosting:**\n",
    "- Decision Stumps (shallow decision trees)\n",
    "- Logistic Regression\n",
    "\n",
    "### Why Use Boosting?\n",
    "\n",
    "- ✔ Reduces bias (improves weak classifiers)\n",
    "- ✔ Lowers variance (reduces overfitting)\n",
    "- ✔ Outperforms individual models\n",
    "- ✔ Wins machine learning competitions (Kaggle, KDD Cup, etc.)\n",
    "\n",
    "## The Idea Behind Boosting\n",
    "\n",
    "Instead of training a single complex model, boosting trains multiple simple models and combines their outputs.\n",
    "\n",
    "**Example: Loan Classification**\n",
    "- Classifier 1: Splits on income\n",
    "- Classifier 2: Splits on credit history\n",
    "- Classifier 3: Splits on loan term\n",
    "- Final Prediction: Weighted combination of all classifiers\n",
    "\n",
    "### Ensemble Learning: The Core of Boosting\n",
    "\n",
    "- Instead of relying on a single decision tree, boosting combines multiple models.\n",
    "- Each model corrects errors made by the previous one.\n",
    "- Final decision = weighted vote of all classifiers.\n",
    "\n",
    "## How Boosting Works\n",
    "\n",
    "1. Train a weak classifier (e.g., decision stump) on the dataset.\n",
    "2. Check which examples it misclassified.\n",
    "3. Increase the importance (weight) of misclassified examples.\n",
    "4. Train a new classifier with updated weights.\n",
    "5. Repeat this process for T iterations.\n",
    "6. Combine all classifiers into a strong model.\n",
    "\n",
    "### Boosting vs. Traditional Models\n",
    "\n",
    "| Method              | Strengths                                      | Weaknesses                        |\n",
    "|---------------------|------------------------------------------------|-----------------------------------|\n",
    "| Logistic Regression | Simple, interpretable, works well for linear data | Cannot model complex patterns     |\n",
    "| Decision Trees      | Handles non-linear data, interpretable         | Prone to overfitting              |\n",
    "| Boosting            | High accuracy, reduces bias & variance         | Requires tuning, sensitive to noise |\n",
    "\n",
    "## Understanding the AdaBoost Algorithm\n",
    "\n",
    "AdaBoost (Adaptive Boosting) was one of the first practical boosting algorithms.\n",
    "\n",
    "- Developed by Freund & Schapire in 1999.\n",
    "- Works by iteratively improving weak classifiers.\n",
    "- Uses a weighted voting system to combine weak models.\n",
    "\n",
    "### AdaBoost Algorithm\n",
    "\n",
    "1. Initialize equal weights for all training examples.\n",
    "2. Train a weak classifier $ f_t(x) $ on the weighted dataset.\n",
    "3. Compute the weighted error:\n",
    "   $\n",
    "   \\text{error} = \\sum (\\text{weight of misclassified points})\n",
    "   $\n",
    "4. Compute the classifier’s importance:\n",
    "   $\n",
    "   w_t = \\frac{1}{2} \\ln \\left( \\frac{1 - \\text{error}}{\\text{error}} \\right)\n",
    "   $\n",
    "5. Update example weights:\n",
    "   - Increase weight of misclassified points.\n",
    "   - Decrease weight of correctly classified points.\n",
    "6. Repeat for T iterations.\n",
    "7. Final prediction is based on weighted sum of classifiers.\n",
    "\n",
    "## How AdaBoost Improves Accuracy\n",
    "\n",
    "**Example: Face Detection**\n",
    "\n",
    "- Classifier 1 detects edges.\n",
    "- Classifier 2 detects eyes.\n",
    "- Classifier 3 detects nose & mouth.\n",
    "- Final ensemble model detects faces with high accuracy.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- Early classifiers handle simple cases.\n",
    "- Later classifiers correct mistakes.\n",
    "- Final model is highly accurate.\n",
    "\n",
    "## Why Does AdaBoost Work?\n",
    "\n",
    "- Focuses on hard-to-classify examples.\n",
    "- Combines multiple weak models into a strong one.\n",
    "- Automatically assigns higher weights to better classifiers.\n",
    "\n",
    "### AdaBoost Theorem\n",
    "\n",
    "- Guarantees that training error decreases to zero as iterations increase.\n",
    "- However, if boosting runs too long, it may overfit.\n",
    "\n",
    "## Overfitting in Boosting\n",
    "\n",
    "Unlike decision trees, boosting is resistant to overfitting. However, if too many weak classifiers are added, test error may increase.\n",
    "\n",
    "### How to Prevent Overfitting?\n",
    "\n",
    "- ✔ Use cross-validation to choose the optimal number of iterations (T).\n",
    "- ✔ Regularization (limit complexity of weak learners).\n",
    "- ✔ Early stopping (stop training when validation error increases).\n",
    "\n",
    "## Comparison: AdaBoost vs. Random Forest\n",
    "\n",
    "| Method           | How It Works                                             | Strengths                          | Weaknesses                        |\n",
    "|------------------|----------------------------------------------------------|-----------------------------------|-----------------------------------|\n",
    "| AdaBoost         | Sequentially trains weak models, adjusts weights based on mistakes | High accuracy, focuses on hard examples | Sensitive to noise                |\n",
    "| Random Forest    | Trains multiple trees in parallel, each on a random subset of data | Robust to noise, easy to parallelize | May require more trees than boosting |\n",
    "| Gradient Boosting| Like AdaBoost but uses gradient descent to optimize      | Even better performance than AdaBoost | More computationally expensive     |\n",
    "\n",
    "## Boosting in the Real World\n",
    "\n",
    "Boosting is widely used in:\n",
    "- ✔ Computer Vision – Face detection, object recognition.\n",
    "- ✔ Search Engines – Ranking web pages.\n",
    "- ✔ Fraud Detection – Identifying credit card fraud.\n",
    "- ✔ Recommender Systems – Netflix movie recommendations.\n",
    "- ✔ Finance & Healthcare – Loan approvals, disease prediction.\n",
    "\n",
    "### Kaggle Competitions\n",
    "\n",
    "Boosting wins over 50% of machine learning competitions.\n",
    "\n",
    "**Popular libraries:**\n",
    "- XGBoost (Extreme Gradient Boosting) – Fast & powerful.\n",
    "- LightGBM – Scalable boosting for large datasets.\n",
    "- CatBoost – Optimized for categorical data.\n",
    "\n",
    "\n",
    "\n",
    "# Why Accuracy is Not Enough?\n",
    "\n",
    "Accuracy is often misleading, especially in imbalanced datasets.\n",
    "\n",
    "### Example: The Problem with Accuracy\n",
    "Suppose 90% of restaurant reviews are negative. A classifier that always predicts \"negative\" achieves 90% accuracy – but it’s useless! It never identifies positive reviews, which are essential for a marketing campaign.\n",
    "\n",
    "**Solution:** Use **Precision & Recall**, which provide a more meaningful evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## The Restaurant Review Example\n",
    "\n",
    "Suppose a restaurant wants to automatically highlight positive reviews to attract customers.\n",
    "\n",
    "- **Goal:** Extract positive sentences from user reviews and display them on the website.\n",
    "- **Model:** A sentiment classifier that predicts whether a sentence is positive or negative.\n",
    "- **Problem:** How do we trust this classifier?\n",
    "  - If it fails, it might post a negative review on the website (very bad for business!).\n",
    "  - **Accuracy alone doesn’t tell us how reliable it is.**\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding Precision and Recall\n",
    "\n",
    "To evaluate a classifier, we need two key metrics:\n",
    "\n",
    "| Metric     | Definition | Why It Matters? |\n",
    "|------------|----------------------------------|----------------------------------------------------------|\n",
    "| **Precision** | Out of all sentences predicted as positive, how many are actually positive? | Ensures we don’t show negative reviews on the website. |\n",
    "| **Recall** | Out of all actual positive sentences, how many did we correctly find? | Ensures we don’t miss good reviews. |\n",
    "\n",
    "### Example: Precision & Recall in Action\n",
    "\n",
    "A classifier identifies 6 sentences as \"positive\".\n",
    "- **Reality:**\n",
    "  - 4 of them are truly positive (✅).\n",
    "  - 2 are actually negative (❌ - false positives).\n",
    "  - Another 2 positive sentences were missed (❌ - false negatives).\n",
    "\n",
    "#### 📌 Precision Calculation:\n",
    "\n",
    "$\n",
    "Precision = \\frac{True\\ Positives}{True\\ Positives + False\\ Positives} = \\frac{4}{6} = 0.67\n",
    "$\n",
    "\n",
    "**Interpretation:** 67% of displayed sentences are actually positive (the rest are mistakes).\n",
    "\n",
    "#### 📌 Recall Calculation:\n",
    "\n",
    "$\n",
    "Recall = \\frac{True\\ Positives}{True\\ Positives + False\\ Negatives} = \\frac{4}{6} = 0.67\n",
    "$\n",
    "\n",
    "**Interpretation:** The model found 67% of all possible positive reviews (but missed some).\n",
    "\n",
    "---\n",
    "\n",
    "## Precision vs. Recall Trade-off\n",
    "\n",
    "- **High Precision, Low Recall:** The model is very selective (only picks the safest positive reviews).\n",
    "- **High Recall, Low Precision:** The model finds most positive reviews but also includes some mistakes.\n",
    "- **Optimizing both is difficult** – you usually have to trade one for the other.\n",
    "\n",
    "### Real-World Analogy: Spam Filter\n",
    "\n",
    "| Scenario | Effect |\n",
    "|------------|-----------------------------------------------------|\n",
    "| **High Precision, Low Recall** | Blocks only obvious spam, but some spam still gets through. |\n",
    "| **High Recall, Low Precision** | Blocks all spam but also blocks important emails. |\n",
    "\n",
    "---\n",
    "\n",
    "## False Positives vs. False Negatives\n",
    "\n",
    "Errors in classification come in two types:\n",
    "\n",
    "| Error Type | Definition | Example (Restaurant Reviews) | Impact |\n",
    "|------------|----------------------------------|-----------------------------------------------------|-------------------------------------|\n",
    "| **False Positive (FP)** | Predict positive, but it’s actually negative. | “The sushi was awful” is mistakenly posted as a positive review. | **Big problem!** Could damage business reputation. |\n",
    "| **False Negative (FN)** | Predict negative, but it’s actually positive. | “The sushi was amazing” is ignored. | **Lost opportunity** to attract customers. |\n",
    "\n",
    "For the restaurant website, **false positives are worse** (we don’t want bad reviews on the site!). Thus, **high precision is more important than high recall.**\n",
    "\n",
    "---\n",
    "\n",
    "## Optimizing the Trade-off: Adjusting the Threshold\n",
    "\n",
    "Most classifiers predict a **probability score** (e.g., 0.99 → highly positive, 0.55 → uncertain).\n",
    "\n",
    "- **Decision Threshold** (default = 0.5) determines when to classify as positive.\n",
    "\n",
    "### How Adjusting the Threshold Changes Precision & Recall\n",
    "\n",
    "| Threshold | Effect | Classifier Behavior |\n",
    "|------------|-------------------------------|--------------------------------------------------|\n",
    "| **T = 0.99 (Very High)** | High Precision, Low Recall | Only very confident predictions are labeled positive. |\n",
    "| **T = 0.50 (Balanced)** | Moderate Precision & Recall | Standard decision boundary. |\n",
    "| **T = 0.01 (Very Low)** | Low Precision, High Recall | Almost everything is classified as positive. |\n",
    "\n",
    "🚀 **Application:**\n",
    "- If we want **high precision** (avoid false positives), use a **higher threshold**.\n",
    "- If we want **high recall** (find all positive reviews), use a **lower threshold**.\n",
    "\n",
    "---\n",
    "\n",
    "## Precision-Recall Curve\n",
    "\n",
    "The **Precision-Recall Curve** shows the trade-off between precision & recall at different thresholds.\n",
    "\n",
    "- **Better classifiers** have curves closer to the top-right corner (high precision and recall).\n",
    "- We compare models using Precision-Recall curves instead of a single accuracy value.\n",
    "\n",
    "### Example: Comparing Two Classifiers\n",
    "\n",
    "- **Classifier A** is better overall (higher precision at every recall level).\n",
    "- **Classifier B** is only better for high recall cases.\n",
    "\n",
    "📌 **Choosing the Best Model:**\n",
    "- If we care about **precision** (avoiding false positives) → Pick **Classifier A**.\n",
    "- If we care about **recall** (finding all positives) → Pick **Classifier B**.\n",
    "\n",
    "---\n",
    "\n",
    "## Precision@K: A Practical Metric\n",
    "\n",
    "**Precision@K** measures precision for the **top K** predictions (e.g., top 5 sentences displayed on a website).\n",
    "\n",
    "### Example:\n",
    "- We show **5 reviews**.\n",
    "- **4 are correct, 1 is negative** → **Precision@5 = 4/5 = 0.8**.\n",
    "\n",
    "### This metric is useful for:\n",
    "- **Search engines** (top 10 results should be relevant).\n",
    "- **Recommender systems** (top 5 recommended products should be useful).\n",
    "- **Chatbot responses** (only show the best replies).\n",
    "\n",
    "# The Challenge of Large Datasets\n",
    "\n",
    "Machine learning models must handle massive amounts of data:\n",
    "\n",
    "- **4.8 billion** web pages\n",
    "- **500 million** tweets per day\n",
    "- **YouTube:** 300 hours of video uploaded every minute\n",
    "\n",
    "Traditional learning algorithms struggle because they require multiple passes over the entire dataset before updating parameters.\n",
    "\n",
    "### 📌 Example: YouTube Ads\n",
    "\n",
    "YouTube must decide in milliseconds which ad to show to a user. This requires fast, scalable machine learning algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Traditional Gradient Descent Fails on Big Data\n",
    "\n",
    "Gradient Descent (GD) requires computing gradients over all data points before making an update.\n",
    "\n",
    "### Problem: If the dataset has billions of examples, this is too slow.\n",
    "\n",
    "#### 📌 Computation Cost Example\n",
    "\n",
    "Suppose each gradient computation takes 1ms:\n",
    "\n",
    "- **1,000** data points → **1 second** (manageable)\n",
    "- **10 million** data points → **2.8 hours** (too slow)\n",
    "- **10 billion** data points → **115.7 days** (impossible!)\n",
    "\n",
    "**Solution?** We need an algorithm that updates faster and doesn’t require scanning the full dataset every time.\n",
    "\n",
    "---\n",
    "\n",
    "## Stochastic Gradient Descent (SGD) – A Game Changer\n",
    "\n",
    "SGD updates parameters more frequently by using only one data point at a time instead of the entire dataset.\n",
    "\n",
    "Instead of computing exact gradients, SGD approximates them using small, random samples.\n",
    "\n",
    "### How SGD Works\n",
    "\n",
    "1. Pick a random data point.\n",
    "2. Compute its gradient.\n",
    "3. Update the model’s parameters.\n",
    "4. Repeat for the next random data point.\n",
    "5. Continue until convergence.\n",
    "\n",
    "### Why is this better?\n",
    "\n",
    "✔ Much faster updates (real-time processing possible).\n",
    "✔ Scales well to massive datasets.\n",
    "✔ Works even if data is streaming.\n",
    "✔ Allows training complex models efficiently (e.g., deep learning).\n",
    "\n",
    "### Trade-offs of SGD\n",
    "\n",
    "❌ Noisy updates (since it's based on a single data point).\n",
    "❌ Oscillations around the optimal solution.\n",
    "❌ More sensitive to hyperparameters (step size, learning rate).\n",
    "\n",
    "---\n",
    "\n",
    "## Comparing Gradient Descent vs. Stochastic Gradient Descent\n",
    "\n",
    "| Method | Pros | Cons |\n",
    "|------------|----------------------------------|--------------------------------|\n",
    "| **Batch Gradient Descent** | Stable convergence, exact gradients | Very slow on big data |\n",
    "| **Stochastic Gradient Descent (SGD)** | Faster updates, handles big data | Noisy updates, oscillates around the solution |\n",
    "\n",
    "📌 **Key Insight:** SGD is almost always faster for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## The Role of Online Learning\n",
    "\n",
    "- **Traditional ML (Batch Learning):** Train on a fixed dataset.\n",
    "- **Online Learning:** Continuously updates the model as new data arrives.\n",
    "\n",
    "### 📌 Example: Online Learning in Ad Targeting\n",
    "\n",
    "1. A user visits a webpage.\n",
    "2. The system predicts which ad the user will click.\n",
    "3. The user clicks (or doesn’t) on an ad.\n",
    "4. The model immediately updates based on this feedback.\n",
    "\n",
    "✔ Always up-to-date with the latest trends.\n",
    "✔ Can handle rapidly changing data streams (e.g., stock prices, social media).\n",
    "❌ More difficult to implement and tune.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Challenges with SGD & Online Learning\n",
    "\n",
    "### Shuffling Data is Crucial\n",
    "\n",
    "- If training data is sorted (e.g., all negative examples first), SGD may learn bad patterns.\n",
    "- **Solution:** Shuffle data before training.\n",
    "\n",
    "### Choosing the Right Learning Rate (Step Size)\n",
    "\n",
    "- **Too small** → Slow learning.\n",
    "- **Too large** → Model oscillates and never converges.\n",
    "- **Solution:** Use a **decaying learning rate**:\n",
    "\n",
    "$\n",
    "\\eta_t = \\frac{\\eta_0}{1+t}\n",
    "$\n",
    "\n",
    "✔ Starts with large updates.\n",
    "✔ Gradually reduces update size over time.\n",
    "\n",
    "### SGD Doesn't Fully Converge (Oscillations)\n",
    "\n",
    "- Unlike gradient descent, SGD never truly stops.\n",
    "- Instead, it bounces around the optimal solution.\n",
    "- **Solution:** Averaging the last few iterations for stability.\n",
    "\n",
    "### Mini-Batch SGD: A Compromise\n",
    "\n",
    "- Instead of **1 data point per update**, use **small batches** (e.g., 32 or 128).\n",
    "- Reduces noise while keeping updates fast.\n",
    "- **Used heavily in Deep Learning (Neural Networks).**\n",
    "\n",
    "---\n",
    "\n",
    "## Distributed & Parallel Machine Learning\n",
    "\n",
    "Big datasets require big compute power.\n",
    "\n",
    "### Techniques to scale ML to massive datasets:\n",
    "\n",
    "- **GPUs & TPUs** (used for Deep Learning).\n",
    "- **Parallel Processing** (splitting data across multiple machines).\n",
    "- **Distributed ML frameworks** (e.g., TensorFlow, PyTorch, Apache Spark MLlib).\n",
    "\n",
    "### 📌 Example: Google Search\n",
    "\n",
    "- Processes **trillions** of queries.\n",
    "- Uses **distributed machine learning** to rank results in milliseconds.\n",
    "\n",
    "# Clustering and Retrieval\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "1. [Course Overview](#course-overview)\n",
    "2. [Nearest Neighbor Search: The Basics](#1-nearest-neighbor-search-the-basics)\n",
    "    - [(A) Data Representation: How Do We Represent a Document?](#a-data-representation-how-do-we-represent-a-document)\n",
    "    - [(B) Distance Metrics: How Do We Compare Documents?](#b-distance-metrics-how-do-we-compare-documents)\n",
    "3. [Critical Components of Nearest Neighbor Search](#2-critical-components-of-nearest-neighbor-search)\n",
    "4. [The Complexity of Nearest Neighbor Search](#3-the-complexity-of-nearest-neighbor-search)\n",
    "5. [Speeding Up Nearest Neighbor Search](#4-speeding-up-nearest-neighbor-search)\n",
    "    - [(A) KD-Trees (Efficient Search for Low/Medium Dimensions)](#a-kd-trees-efficient-search-for-lowmedium-dimensions)\n",
    "    - [(B) Approximate Nearest Neighbor (ANN) with Locality-Sensitive Hashing (LSH)](#b-approximate-nearest-neighbor-ann-with-locality-sensitive-hashing-lsh)\n",
    "6. [Introduction to Clustering](#5-introduction-to-clustering)\n",
    "7. [Clustering as an Unsupervised Learning Task](#6-clustering-as-an-unsupervised-learning-task)\n",
    "8. [K-Means Clustering](#7-k-means-clustering)\n",
    "    - [Basic Algorithm](#basic-algorithm)\n",
    "    - [Key Properties](#key-properties)\n",
    "    - [K-Means++ (Smart Initialization)](#k-means-smart-initialization)\n",
    "9. [Evaluating Clustering Quality & Choosing k](#8-evaluating-clustering-quality--choosing-k)\n",
    "    - [Cluster Heterogeneity](#cluster-heterogeneity)\n",
    "    - [Choosing k](#choosing-k)\n",
    "10. [Parallelizing K-Means Using MapReduce](#9-parallelizing-k-means-using-mapreduce)\n",
    "    - [MapReduce Framework](#mapreduce-framework)\n",
    "    - [Optimizations](#optimizations)\n",
    "11. [Applications of Clustering](#10-applications-of-clustering)\n",
    "12. [Overview](#overview)\n",
    "13. [Why Probabilistic Clustering?](#why-probabilistic-clustering)\n",
    "14. [Mixture Models & Soft Assignments](#mixture-models--soft-assignments)\n",
    "    - [Limitations of K-Means](#limitations-of-k-means)\n",
    "    - [Mixture Models Approach](#mixture-models-approach)\n",
    "15. [Gaussian Mixture Model (GMM)](#gaussian-mixture-model-gmm)\n",
    "    - [Key Equations in GMM](#key-equations-in-gmm)\n",
    "16. [Expectation-Maximization (EM) Algorithm](#expectation-maximization-em-algorithm)\n",
    "    - [Steps of the EM Algorithm](#steps-of-the-em-algorithm)\n",
    "    - [Mathematical Form of the EM Steps](#mathematical-form-of-the-em-steps)\n",
    "    - [Intuition Behind EM](#intuition-behind-em)\n",
    "17. [Comparison: GMM vs. K-Means](#comparison-gmm-vs-k-means)\n",
    "    - [K-Means as a Special Case of GMM](#k-means-as-a-special-case-of-gmm)\n",
    "18. [Challenges & Practical Considerations](#challenges--practical-considerations)\n",
    "    - [Convergence & Initialization](#convergence--initialization)\n",
    "    - [Degeneracy & Overfitting](#degeneracy--overfitting)\n",
    "    - [Computational Complexity](#computational-complexity)\n",
    "19. [Motivation for Mixed Membership Models](#motivation-for-mixed-membership-models)\n",
    "20. [Clustering vs. Mixed Membership Models](#clustering-vs-mixed-membership-models)\n",
    "    - [Clustering Models (e.g., K-means, Gaussian Mixture Models)](#clustering-models-eg-k-means-gaussian-mixture-models)\n",
    "    - [Mixed Membership Models (e.g., LDA)](#mixed-membership-models-eg-lda)\n",
    "21. [Bag-of-Words Representation & Mixture of Multinomials](#bag-of-words-representation--mixture-of-multinomials)\n",
    "    - [Alternative to Mixture of Gaussians](#alternative-to-mixture-of-gaussians)\n",
    "22. [Latent Dirichlet Allocation (LDA)](#latent-dirichlet-allocation-lda)\n",
    "    - [Key Components of LDA](#key-components-of-lda)\n",
    "    - [Comparison to Clustering](#comparison-to-clustering)\n",
    "23. [Inference in LDA: Learning Topics from Data](#inference-in-lda-learning-topics-from-data)\n",
    "24. [Expectation-Maximization (EM) vs. Gibbs Sampling](#expectation-maximization-em-vs-gibbs-sampling)\n",
    "    - [Bayesian Approach to LDA](#bayesian-approach-to-lda)\n",
    "25. [Gibbs Sampling for LDA](#gibbs-sampling-for-lda)\n",
    "    - [What is Gibbs Sampling?](#what-is-gibbs-sampling)\n",
    "    - [Steps of Gibbs Sampling in LDA](#steps-of-gibbs-sampling-in-lda)\n",
    "26. [Collapsed Gibbs Sampling](#collapsed-gibbs-sampling)\n",
    "    - [Optimization Trick: Marginalizing Out Model Parameters](#optimization-trick-marginalizing-out-model-parameters)\n",
    "27. [Applications of LDA](#applications-of-lda)\n",
    "28. [Nearest Neighbor Search & Retrieval](#nearest-neighbor-search--retrieval)\n",
    "29. [Nearest Neighbor Search (NNS)](#nearest-neighbor-search-nns)\n",
    "    - [Key Challenges & Solutions](#key-challenges--solutions)\n",
    "        - [Data Representation](#data-representation)\n",
    "        - [Distance Metrics](#distance-metrics)\n",
    "        - [Scalability Issues](#scalability-issues)\n",
    "    - [Efficient Retrieval Techniques](#efficient-retrieval-techniques)\n",
    "        - [KD-Trees](#kd-trees)\n",
    "        - [Locality-Sensitive Hashing (LSH)](#locality-sensitive-hashing-lsh)\n",
    "30. [Clustering Algorithms](#clustering-algorithms)\n",
    "    - [1. K-Means Clustering](#1-k-means-clustering)\n",
    "        - [Iterative Algorithm](#iterative-algorithm)\n",
    "    - [2. Gaussian Mixture Models (GMMs)](#2-gaussian-mixture-models-gmms)\n",
    "        - [Key Features](#key-features)\n",
    "    - [3. Probabilistic Clustering: Mixture Models](#3-probabilistic-clustering-mixture-models)\n",
    "    - [4. Latent Dirichlet Allocation (LDA) – Topic Modeling](#4-latent-dirichlet-allocation-lda--topic-modeling)\n",
    "        - [Why Mixed Membership Models?](#why-mixed-membership-models)\n",
    "        - [LDA Components](#lda-components)\n",
    "        - [LDA Inference: Gibbs Sampling](#lda-inference-gibbs-sampling)\n",
    "        - [Applications](#applications)\n",
    "    - [5. Hierarchical Clustering](#5-hierarchical-clustering)\n",
    "        - [Why Use Hierarchical Clustering?](#why-use-hierarchical-clustering)\n",
    "        - [Two Main Types](#two-main-types)\n",
    "            - [Divisive Clustering (Top-Down)](#divisive-clustering-top-down)\n",
    "            - [Agglomerative Clustering (Bottom-Up)](#agglomerative-clustering-bottom-up)\n",
    "        - [Linkage Methods](#linkage-methods)\n",
    "        - [Cutting the Dendrogram](#cutting-the-dendrogram)\n",
    "    - [6. Hidden Markov Models (HMMs)](#6-hidden-markov-models-hmms)\n",
    "        - [Why Use HMMs?](#why-use-hmms)\n",
    "        - [HMM Components](#hmm-components)\n",
    "        - [Inference in HMMs](#inference-in-hmms)\n",
    "        - [Applications](#applications)\n",
    "\n",
    "# Course Overview\n",
    "\n",
    "This course is part of a machine learning specialization designed to be taken in sequence. It focuses on two key concepts: clustering and retrieval, both widely used in practical applications.\n",
    "\n",
    "- **Retrieval:** Finding similar items (e.g., recommending similar products, suggesting related news articles, matching social media users).\n",
    "- **Clustering:** Grouping data into meaningful categories (e.g., segmenting customers, detecting topic clusters in documents, categorizing images).\n",
    "\n",
    "Unlike previous courses in the specialization, which covered regression and classification, this course focuses on unsupervised learning techniques.\n",
    "\n",
    "\n",
    "## 1. Nearest Neighbor Search: The Basics\n",
    "\n",
    "**Concept:**\n",
    "\n",
    "Given a query document, find the most similar document in a dataset.\n",
    "Instead of scanning all documents (brute-force), we structure the search to make it efficient.\n",
    "\n",
    "**Formulation:**\n",
    "\n",
    "- **1-Nearest Neighbor (1-NN):** Find the single most similar document.\n",
    "- **k-Nearest Neighbors (k-NN):** Retrieve the top-k most similar documents.\n",
    "\n",
    "**Algorithm for 1-NN:**\n",
    "\n",
    "1. Compute distances from the query document to all other documents.\n",
    "2. Identify the document with the smallest distance (nearest neighbor).\n",
    "\n",
    "**Algorithm for k-NN:**\n",
    "\n",
    "1. Compute distances for all documents.\n",
    "2. Maintain a sorted list of the k closest neighbors.\n",
    "\n",
    "## 2. Critical Components of Nearest Neighbor Search\n",
    "\n",
    "### (A) Data Representation: How Do We Represent a Document?\n",
    "\n",
    "- **Bag of Words (BoW):** A vector of word counts per document.\n",
    "- **TF-IDF (Term Frequency - Inverse Document Frequency):**\n",
    "  - Gives higher importance to rare words in a document.\n",
    "  - Downweights common words like \"the,\" \"and,\" etc.\n",
    "  - Helps refine document similarity calculations.\n",
    "\n",
    "### (B) Distance Metrics: How Do We Compare Documents?\n",
    "\n",
    "- **Euclidean Distance:** Measures absolute distance in high-dimensional space (not ideal for text).\n",
    "- **Cosine Similarity:** Measures the angle between two document vectors (better for text comparison).\n",
    "  - **Key Advantage:** Invariant to document length.\n",
    "  - **Trade-off:** It may make short and long documents seem equally similar when they aren't.\n",
    "- **Hybrid Approaches:** Different distance metrics for different features (e.g., cosine similarity for text, Euclidean for numerical features like view counts).\n",
    "\n",
    "## 3. The Complexity of Nearest Neighbor Search\n",
    "\n",
    "- **Brute-force search (Linear Scan):**\n",
    "  - **1-NN:** $ O(N) $ (N = number of documents).\n",
    "  - **k-NN:** $ O(N \\log k) $ (if implemented with an efficient priority queue).\n",
    "  - **Problem:** Too slow for large datasets (millions/billions of documents).\n",
    "\n",
    "## 4. Speeding Up Nearest Neighbor Search\n",
    "\n",
    "### (A) KD-Trees (Efficient Search for Low/Medium Dimensions)\n",
    "\n",
    "A binary tree that recursively partitions data along dimensions.\n",
    "\n",
    "**Steps to build a KD-tree:**\n",
    "\n",
    "1. Choose a splitting dimension (e.g., word frequency).\n",
    "2. Split the dataset at the median value along that dimension.\n",
    "3. Recursively partition data into subspaces.\n",
    "\n",
    "**KD-tree Search Process:**\n",
    "\n",
    "1. Start at the root and traverse to the leaf containing the query.\n",
    "2. Compare distances within the nearest partition.\n",
    "3. Backtrack and prune partitions that can't contain a closer neighbor.\n",
    "\n",
    "**Complexity:**\n",
    "\n",
    "- **Best case:** $ O(\\log N) $ (highly structured data).\n",
    "- **Worst case:** $ O(N) $ (bad partitioning).\n",
    "\n",
    "**Issues with KD-Trees:**\n",
    "\n",
    "- **High-dimensional failure:**\n",
    "  - In high dimensions, most points are far apart, leading to exponential complexity in $ d $ (curse of dimensionality).\n",
    "  - **Rule of thumb:** Only useful if $ N >> 2^d $.\n",
    "\n",
    "**Solution:** Approximate Nearest Neighbor search.\n",
    "\n",
    "### (B) Approximate Nearest Neighbor (ANN) with Locality-Sensitive Hashing (LSH)\n",
    "\n",
    "**Key Idea:** Instead of exact nearest neighbors, find a “good enough” neighbor quickly.\n",
    "\n",
    "**LSH Process:**\n",
    "\n",
    "1. Randomly project data onto multiple random hyperplanes.\n",
    "2. Assign each point a binary bit-vector based on which side of the hyperplane it falls.\n",
    "3. Store these bit-vectors in a hash table (faster lookup).\n",
    "4. For a query, search its hashed bucket and nearby bins.\n",
    "\n",
    "**Trade-off:**\n",
    "\n",
    "- Much faster than brute force or KD-trees in high dimensions.\n",
    "- Less accuracy, but we control the speed vs. accuracy trade-off.\n",
    "\n",
    "**Advantages of LSH:**\n",
    "\n",
    "- Scales well to large datasets.\n",
    "- Works better than KD-trees in high dimensions.\n",
    "- Has probabilistic guarantees on search quality.\n",
    "\n",
    "# Course Overview\n",
    "\n",
    "This course is part of a machine learning specialization designed to be taken in sequence. It focuses on two key concepts: clustering and retrieval, both widely used in practical applications.\n",
    "\n",
    "- **Retrieval:** Finding similar items (e.g., recommending similar products, suggesting related news articles, matching social media users).\n",
    "- **Clustering:** Grouping data into meaningful categories (e.g., segmenting customers, detecting topic clusters in documents, categorizing images).\n",
    "\n",
    "Unlike previous courses in the specialization, which covered regression and classification, this course focuses on unsupervised learning techniques.\n",
    "\n",
    "## 1. Nearest Neighbor Search: The Basics\n",
    "\n",
    "**Concept:**\n",
    "\n",
    "Given a query document, find the most similar document in a dataset.\n",
    "Instead of scanning all documents (brute-force), we structure the search to make it efficient.\n",
    "\n",
    "**Formulation:**\n",
    "\n",
    "- **1-Nearest Neighbor (1-NN):** Find the single most similar document.\n",
    "- **k-Nearest Neighbors (k-NN):** Retrieve the top-k most similar documents.\n",
    "\n",
    "**Algorithm for 1-NN:**\n",
    "\n",
    "1. Compute distances from the query document to all other documents.\n",
    "2. Identify the document with the smallest distance (nearest neighbor).\n",
    "\n",
    "**Algorithm for k-NN:**\n",
    "\n",
    "1. Compute distances for all documents.\n",
    "2. Maintain a sorted list of the k closest neighbors.\n",
    "\n",
    "## 2. Critical Components of Nearest Neighbor Search\n",
    "\n",
    "### (A) Data Representation: How Do We Represent a Document?\n",
    "\n",
    "- **Bag of Words (BoW):** A vector of word counts per document.\n",
    "- **TF-IDF (Term Frequency - Inverse Document Frequency):**\n",
    "  - Gives higher importance to rare words in a document.\n",
    "  - Downweights common words like \"the,\" \"and,\" etc.\n",
    "  - Helps refine document similarity calculations.\n",
    "\n",
    "### (B) Distance Metrics: How Do We Compare Documents?\n",
    "\n",
    "- **Euclidean Distance:** Measures absolute distance in high-dimensional space (not ideal for text).\n",
    "- **Cosine Similarity:** Measures the angle between two document vectors (better for text comparison).\n",
    "  - **Key Advantage:** Invariant to document length.\n",
    "  - **Trade-off:** It may make short and long documents seem equally similar when they aren't.\n",
    "- **Hybrid Approaches:** Different distance metrics for different features (e.g., cosine similarity for text, Euclidean for numerical features like view counts).\n",
    "\n",
    "## 3. The Complexity of Nearest Neighbor Search\n",
    "\n",
    "- **Brute-force search (Linear Scan):**\n",
    "  - **1-NN:** $ O(N) $ (N = number of documents).\n",
    "  - **k-NN:** $ O(N \\log k) $ (if implemented with an efficient priority queue).\n",
    "  - **Problem:** Too slow for large datasets (millions/billions of documents).\n",
    "\n",
    "## 4. Speeding Up Nearest Neighbor Search\n",
    "\n",
    "### (A) KD-Trees (Efficient Search for Low/Medium Dimensions)\n",
    "\n",
    "A binary tree that recursively partitions data along dimensions.\n",
    "\n",
    "**Steps to build a KD-tree:**\n",
    "\n",
    "1. Choose a splitting dimension (e.g., word frequency).\n",
    "2. Split the dataset at the median value along that dimension.\n",
    "3. Recursively partition data into subspaces.\n",
    "\n",
    "**KD-tree Search Process:**\n",
    "\n",
    "1. Start at the root and traverse to the leaf containing the query.\n",
    "2. Compare distances within the nearest partition.\n",
    "3. Backtrack and prune partitions that can't contain a closer neighbor.\n",
    "\n",
    "**Complexity:**\n",
    "\n",
    "- **Best case:** $ O(\\log N) $ (highly structured data).\n",
    "- **Worst case:** $ O(N) $ (bad partitioning).\n",
    "\n",
    "**Issues with KD-Trees:**\n",
    "\n",
    "- **High-dimensional failure:**\n",
    "  - In high dimensions, most points are far apart, leading to exponential complexity in $ d $ (curse of dimensionality).\n",
    "  - **Rule of thumb:** Only useful if $ N >> 2^d $.\n",
    "\n",
    "**Solution:** Approximate Nearest Neighbor search.\n",
    "\n",
    "### (B) Approximate Nearest Neighbor (ANN) with Locality-Sensitive Hashing (LSH)\n",
    "\n",
    "**Key Idea:** Instead of exact nearest neighbors, find a “good enough” neighbor quickly.\n",
    "\n",
    "**LSH Process:**\n",
    "\n",
    "1. Randomly project data onto multiple random hyperplanes.\n",
    "2. Assign each point a binary bit-vector based on which side of the hyperplane it falls.\n",
    "3. Store these bit-vectors in a hash table (faster lookup).\n",
    "4. For a query, search its hashed bucket and nearby bins.\n",
    "\n",
    "**Trade-off:**\n",
    "\n",
    "- Much faster than brute force or KD-trees in high dimensions.\n",
    "- Less accuracy, but we control the speed vs. accuracy trade-off.\n",
    "\n",
    "**Advantages of LSH:**\n",
    "\n",
    "- Scales well to large datasets.\n",
    "- Works better than KD-trees in high dimensions.\n",
    "- Has probabilistic guarantees on search quality.\n",
    "\n",
    "## 5. Introduction to Clustering\n",
    "\n",
    "Unlike traditional document retrieval, which finds similar documents based on input queries, clustering is about discovering structure in data.\n",
    "Goal: Group similar documents (or data points) without predefined labels.\n",
    "Example: Articles about sports, world news, or entertainment are grouped automatically without explicit category labels.\n",
    "\n",
    "## 6. Clustering as an Unsupervised Learning Task\n",
    "\n",
    "- **Supervised Learning:** Has labeled training data (e.g., house price prediction, sentiment analysis).\n",
    "- **Unsupervised Learning (Clustering):** No labels; the algorithm must discover structure.\n",
    "\n",
    "**Example:** Given word count features of documents, the model groups them based on similarity.\n",
    "\n",
    "**Cluster Representation:**\n",
    "\n",
    "- Each cluster is defined by its centroid (mean position) and its shape (e.g., ellipses).\n",
    "- Assignment is based on distance to cluster centroids.\n",
    "- The process is iterative—assign points → update centroids → repeat.\n",
    "\n",
    "## 7. K-Means Clustering\n",
    "\n",
    "### Basic Algorithm\n",
    "\n",
    "1. Initialize k cluster centers randomly.\n",
    "2. Assign each data point to the closest cluster (using Euclidean distance).\n",
    "3. Update cluster centroids to be the mean of points in the cluster.\n",
    "4. Repeat steps 2 & 3 until convergence.\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "- **Centroid-Based:** Uses mean to determine cluster centers.\n",
    "- **Partitioning Method:** Each data point is assigned to exactly one cluster.\n",
    "- **Sensitive to Initialization:** Poor starting points lead to suboptimal solutions.\n",
    "\n",
    "### K-Means++ (Smart Initialization)\n",
    "\n",
    "Instead of random initialization, K-Means++:\n",
    "- Picks the first centroid randomly.\n",
    "- Selects subsequent centroids proportional to squared distance from existing centroids.\n",
    "- Ensures better separation of clusters.\n",
    "- Leads to faster convergence and better clustering.\n",
    "\n",
    "## 8. Evaluating Clustering Quality & Choosing k\n",
    "\n",
    "### Cluster Heterogeneity\n",
    "\n",
    "Sum of squared distances between points and their assigned centroid.\n",
    "Lower heterogeneity = better clustering.\n",
    "\n",
    "### Choosing k:\n",
    "\n",
    "- **Elbow Method:** Plot heterogeneity vs. k. Choose k at the point where adding more clusters results in diminishing returns.\n",
    "  - Too small k: Overly broad clusters.\n",
    "  - Too large k: Overfitting (clusters too small to be meaningful).\n",
    "\n",
    "## 9. Parallelizing K-Means Using MapReduce\n",
    "\n",
    "Since K-Means needs to process large-scale data, we can distribute it using MapReduce.\n",
    "\n",
    "### MapReduce Framework\n",
    "\n",
    "- **Map Phase (Classification Step):**\n",
    "  - Assigns each data point to the closest cluster center.\n",
    "  - Emits (cluster label, data point) pairs.\n",
    "- **Reduce Phase (Recenter Step):**\n",
    "  - Aggregates data points for each cluster.\n",
    "  - Computes new centroids (mean of points in each cluster).\n",
    "\n",
    "**Iterative Process:** Since K-Means is an iterative algorithm, MapReduce needs to be run multiple times until convergence.\n",
    "\n",
    "### Optimizations:\n",
    "\n",
    "- **Combiner Step:** Local aggregation before reducing, reducing network communication.\n",
    "- **Efficient Data Partitioning:** Ensure balanced workload across machines.\n",
    "\n",
    "## 10. Applications of Clustering\n",
    "\n",
    "Clustering is widely used across industries:\n",
    "\n",
    "1. **Information Retrieval**\n",
    "   - Google News: Groups similar articles together.\n",
    "   - Search Engines: Clusters documents to improve recommendations.\n",
    "2. **Image Clustering**\n",
    "   - Google Images Search: Clusters similar images.\n",
    "   - Ambiguous Queries (e.g., \"Cardinal\"): Groups images into categories (bird, baseball team, religious figure).\n",
    "3. **Healthcare & Medicine**\n",
    "   - Patient Segmentation: Identifies subgroups with similar medical conditions.\n",
    "   - Seizure Classification: Groups seizure types for better treatment.\n",
    "4. **E-commerce & Recommendations**\n",
    "   - Amazon Product Clustering: Groups products based on purchase history.\n",
    "   - User Clustering: Groups users with similar buying behavior for personalized recommendations.\n",
    "5. **Crime Forecasting & Housing Prices**\n",
    "   - Crime Prediction: Clusters geographic regions with similar crime patterns to improve forecasting.\n",
    "   - Housing Market Analysis: Groups similar neighborhoods for better price predictions.\n",
    "\n",
    "# Overview\n",
    "\n",
    "The lecture extends K-means clustering by introducing probabilistic model-based clustering, specifically Mixture Models and the Expectation-Maximization (EM) algorithm. The motivation is to address K-means' limitations, such as:\n",
    "\n",
    "- Hard assignments of data points to clusters\n",
    "- Assumption of equal-sized, spherical clusters\n",
    "- Inability to handle overlapping or elongated clusters\n",
    "\n",
    "## Why Probabilistic Clustering?\n",
    "\n",
    "- Real-world data is often not clearly separable.\n",
    "- Some points may belong to multiple clusters with varying probabilities.\n",
    "- K-means ignores cluster shapes and assumes equal importance for all dimensions.\n",
    "\n",
    "## Mixture Models & Soft Assignments\n",
    "\n",
    "### Limitations of K-Means\n",
    "\n",
    "- **Hard assignments:** A point must belong to one cluster, ignoring uncertainty.\n",
    "- **Fixed cluster shapes:** K-means assumes all clusters are equally spread.\n",
    "- **Inefficiency in overlapping clusters:** K-means cannot express confidence levels in assignments.\n",
    "\n",
    "### Mixture Models Approach\n",
    "\n",
    "A Mixture Model allows for soft assignments, meaning each data point is assigned a probability of belonging to each cluster. This is particularly useful in:\n",
    "\n",
    "- Document clustering, where articles may belong to multiple topics.\n",
    "- Image clustering, where images might share characteristics with multiple categories.\n",
    "\n",
    "**Example:**\n",
    "An article could have:\n",
    "- 54% probability of belonging to the \"World News\" cluster\n",
    "- 45% probability of belonging to the \"Science\" cluster\n",
    "- 1% probability of belonging to \"Sports\"\n",
    "\n",
    "## Gaussian Mixture Model (GMM)\n",
    "\n",
    "A Mixture of Gaussians assumes that each cluster follows a Gaussian (Normal) distribution. Instead of just defining cluster centers (like in K-means), we define:\n",
    "\n",
    "- **Mean (μ):** The center of the Gaussian cluster.\n",
    "- **Covariance matrix (Σ):** Defines the shape, spread, and orientation of the cluster.\n",
    "- **Cluster weight (π):** Probability that a randomly selected data point belongs to a given cluster.\n",
    "\n",
    "This allows GMM to model elliptical and overlapping clusters rather than just spherical ones.\n",
    "\n",
    "### Key Equations in GMM\n",
    "\n",
    "- **Cluster probability (prior probability):**\n",
    "  $\n",
    "  P(Z_i = k) = \\pi_k\n",
    "  $\n",
    "  where $\\pi_k$ represents the weight of cluster $k$.\n",
    "\n",
    "- **Likelihood of data given a cluster:**\n",
    "  $\n",
    "  P(X_i \\mid Z_i = k) = N(X_i \\mid \\mu_k, \\Sigma_k)\n",
    "  $\n",
    "  where $\\mu_k$ and $\\Sigma_k$ define the cluster distribution.\n",
    "\n",
    "- **Bayes Rule for Soft Assignments (Responsibilities):**\n",
    "  $\n",
    "  P(Z_i = k \\mid X_i) = \\frac{\\pi_k N(X_i \\mid \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j N(X_i \\mid \\mu_j, \\Sigma_j)}\n",
    "  $\n",
    "  This equation determines how much responsibility each cluster takes for a data point.\n",
    "\n",
    "## Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "Since both cluster assignments and parameters (means, covariances, weights) are unknown, we use the Expectation-Maximization (EM) algorithm to estimate them iteratively.\n",
    "\n",
    "### Steps of the EM Algorithm\n",
    "\n",
    "1. Initialize cluster parameters (randomly or via K-means).\n",
    "2. **E-Step (Expectation Step):** Compute the responsibilities (soft assignments) using the current parameters.\n",
    "3. **M-Step (Maximization Step):** Recompute cluster parameters ($\\pi$, $\\mu$, $\\Sigma$) using the soft assignments.\n",
    "4. Repeat until convergence (i.e., parameters stabilize).\n",
    "\n",
    "### Mathematical Form of the EM Steps\n",
    "\n",
    "- **E-Step:** Compute responsibilities using Bayes' rule:\n",
    "  $\n",
    "  r_{ik} = \\frac{\\pi_k N(X_i \\mid \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j N(X_i \\mid \\mu_j, \\Sigma_j)}\n",
    "  $\n",
    "\n",
    "- **M-Step:** Update cluster parameters:\n",
    "  - **New cluster weights ($\\pi$):**\n",
    "    $\n",
    "    \\pi_k = \\frac{1}{N} \\sum_{i=1}^{N} r_{ik}\n",
    "    $\n",
    "  - **New cluster means ($\\mu$):**\n",
    "    $\n",
    "    \\mu_k = \\frac{\\sum_{i=1}^{N} r_{ik} X_i}{\\sum_{i=1}^{N} r_{ik}}\n",
    "    $\n",
    "  - **New covariance matrices ($\\Sigma$):**\n",
    "    $\n",
    "    \\Sigma_k = \\frac{\\sum_{i=1}^{N} r_{ik} (X_i - \\mu_k)(X_i - \\mu_k)^T}{\\sum_{i=1}^{N} r_{ik}}\n",
    "    $\n",
    "\n",
    "### Intuition Behind EM\n",
    "\n",
    "- The **E-Step** estimates how likely each point belongs to each cluster.\n",
    "- The **M-Step** updates cluster parameters based on these probabilities.\n",
    "- Iteratively refines clusters until convergence.\n",
    "\n",
    "## Comparison: GMM vs. K-Means\n",
    "\n",
    "| Feature                  | K-Means                          | Gaussian Mixture Model (GMM)       |\n",
    "|--------------------------|----------------------------------|------------------------------------|\n",
    "| Cluster Assignments      | Hard (one cluster per point)     | Soft (probabilistic)               |\n",
    "| Cluster Shape            | Spherical (equal variance)       | Elliptical (covariance accounts for shape) |\n",
    "| Handles Overlapping Clusters? | No                           | Yes                                |\n",
    "| Probability Estimates?   | No                               | Yes                                |\n",
    "| Optimization Method      | Lloyd’s Algorithm (iterative)    | Expectation-Maximization (EM)      |\n",
    "\n",
    "### K-Means as a Special Case of GMM\n",
    "\n",
    "If we fix the covariance matrices $\\Sigma_k$ to be equal and drive variances to zero, GMM reduces to K-means. This explains why K-means is a special case of GMM with simplified assumptions.\n",
    "\n",
    "## Challenges & Practical Considerations\n",
    "\n",
    "### Convergence & Initialization\n",
    "\n",
    "- EM only guarantees local convergence (not a global optimum).\n",
    "- Good initialization (e.g., K-means++ initialization) improves results.\n",
    "- Log-likelihood can be monitored to track convergence.\n",
    "\n",
    "### Degeneracy & Overfitting\n",
    "\n",
    "- **Collapsing clusters:** A single point may form a cluster with zero variance, causing infinite likelihood.\n",
    "  - **Solution:** Regularize by adding a small value to covariance matrices (Laplace smoothing).\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "- GMM is slower than K-means due to matrix inversions in covariance estimation.\n",
    "- **Trade-off:** Higher computational cost for better clustering flexibility.\n",
    "\n",
    "# Motivation for Mixed Membership Models\n",
    "\n",
    "## Clustering vs. Mixed Membership Models\n",
    "\n",
    "### Clustering Models (e.g., K-means, Gaussian Mixture Models)\n",
    "\n",
    "- Group similar articles into disjoint clusters.\n",
    "- Assign each document to a single topic (hard or soft assignment).\n",
    "\n",
    "**Example:** A document about epileptic events might be classified into science (Cluster 4) or technology (Cluster 2) but not both.\n",
    "\n",
    "### Mixed Membership Models (e.g., LDA)\n",
    "\n",
    "- Allow documents to belong to multiple topics with different proportions.\n",
    "- **Example:** A document could be 40% science, 60% technology.\n",
    "- Each word in the document can be assigned to a different topic.\n",
    "\n",
    "This approach is more realistic for tasks like:\n",
    "- News categorization (one article can belong to multiple sections).\n",
    "- User preference modeling (users read content from multiple domains).\n",
    "- Document retrieval (retrieving articles based on topic mixture similarity).\n",
    "\n",
    "## Bag-of-Words Representation & Mixture of Multinomials\n",
    "\n",
    "### Alternative to Mixture of Gaussians\n",
    "\n",
    "Instead of representing documents using tf-idf vectors and modeling them with Gaussian Mixtures, LDA uses:\n",
    "\n",
    "- **Bag-of-Words Representation:** Treats a document as a multiset (unordered list) of words.\n",
    "- **Multinomial Distribution:** Each topic is modeled as a probability distribution over words.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "| Topic      | Top Words                          |\n",
    "|------------|------------------------------------|\n",
    "| Science    | brain, neuron, experiment, physics |\n",
    "| Technology | model, algorithm, system, network  |\n",
    "| Sports     | game, player, score, football      |\n",
    "\n",
    "This approach allows documents to be generated from multiple topics rather than just one.\n",
    "\n",
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "### Key Components of LDA\n",
    "\n",
    "- **Topic-Specific Vocabulary Distributions (Word Probabilities):** Each topic is represented by a probability distribution over words.\n",
    "  - **Example:** The word \"neuron\" is highly probable in the \"Science\" topic but unlikely in \"Sports.\"\n",
    "- **Document-Specific Topic Distributions (Topic Proportions):** Each document has a probability distribution over topics.\n",
    "  - **Example:** A science-technology article might be 70% Science, 30% Technology.\n",
    "- **Word Assignments (Hidden Variables):** Each word in a document is assigned to a specific topic.\n",
    "  - **Example:** In a sentence, \"The neural network model improved\", the word \"neural\" might be assigned to \"Science\" and \"model\" to \"Technology.\"\n",
    "\n",
    "### Comparison to Clustering\n",
    "\n",
    "| Feature                        | Clustering (e.g., K-Means) | LDA (Mixed Membership)          |\n",
    "|--------------------------------|----------------------------|---------------------------------|\n",
    "| Assignment                     | One topic per document     | Multiple topics per document    |\n",
    "| Uncertainty                    | No uncertainty captured    | Soft topic proportions per document |\n",
    "| Word-Level Topic Assignment    | No                         | Yes (each word is assigned a topic) |\n",
    "\n",
    "## Inference in LDA: Learning Topics from Data\n",
    "\n",
    "Since we only observe words, LDA needs to infer:\n",
    "- Topic-word distributions (which words belong to which topics).\n",
    "- Document-topic distributions (which topics are present in each document).\n",
    "- Word-topic assignments (which topic each word belongs to).\n",
    "\n",
    "The challenge is that we don’t know these parameters beforehand. We need inference algorithms to estimate them.\n",
    "\n",
    "## Expectation-Maximization (EM) vs. Gibbs Sampling\n",
    "\n",
    "LDA could be solved with EM (Expectation-Maximization), but:\n",
    "- MLE-based estimation overfits in high-dimensional spaces.\n",
    "- The E-step becomes intractable due to complex probability distributions.\n",
    "\n",
    "### Bayesian Approach to LDA\n",
    "\n",
    "LDA is typically solved using a Bayesian approach, which:\n",
    "- Accounts for uncertainty in model parameters.\n",
    "- Regularizes estimates to avoid overfitting.\n",
    "- Uses Gibbs Sampling instead of EM.\n",
    "\n",
    "## Gibbs Sampling for LDA\n",
    "\n",
    "### What is Gibbs Sampling?\n",
    "\n",
    "A Markov Chain Monte Carlo (MCMC) method that iteratively refines topic assignments. Instead of computing the exact posterior, it samples values from conditional distributions.\n",
    "\n",
    "- Randomly reassigns each word to a topic, considering:\n",
    "  - How prevalent the topic is in the document.\n",
    "  - How likely the topic is to generate the word.\n",
    "\n",
    "### Steps of Gibbs Sampling in LDA\n",
    "\n",
    "1. Initialize topic assignments randomly for each word in the corpus.\n",
    "2. For each word in each document:\n",
    "   - Remove its current assignment.\n",
    "   - Compute probability of assigning it to each topic using:\n",
    "     $\n",
    "     P(Z_{iw} = k \\mid \\text{other assignments}) \\propto P(\\text{topic } k \\mid \\text{document } i) \\times P(\\text{word } w \\mid \\text{topic } k)\n",
    "     $\n",
    "   - Sample a new topic assignment from this distribution.\n",
    "   - Update topic counts.\n",
    "3. Repeat until convergence or computational budget is exhausted.\n",
    "\n",
    "**Example: Resampling a Word**\n",
    "\n",
    "Suppose we are reassigning the word \"neuron\":\n",
    "- If the Science topic has many words in this document, $ P(\\text{Science} \\mid \\text{Document}) $ is high.\n",
    "- If \"neuron\" appears frequently in Science articles, $ P(\\text{\"neuron\"} \\mid \\text{Science}) $ is high.\n",
    "- The new topic is sampled based on the product of these probabilities.\n",
    "\n",
    "## Collapsed Gibbs Sampling\n",
    "\n",
    "### Optimization Trick: Marginalizing Out Model Parameters\n",
    "\n",
    "Instead of sampling the topic-word and document-topic distributions separately, we integrate them out. This simplifies the problem to sampling only the word-topic assignments.\n",
    "\n",
    "**Benefits:**\n",
    "- Faster convergence.\n",
    "- Reduces parameter space.\n",
    "- More efficient for large-scale data.\n",
    "\n",
    "**Trade-offs:**\n",
    "- Collapsed Gibbs Sampling eliminates the need to store large probability distributions.\n",
    "- But it requires sequential processing, making it harder to parallelize.\n",
    "\n",
    "## Applications of LDA\n",
    "\n",
    "1. **Topic Discovery in Large Text Corpora**\n",
    "   - Uncover hidden themes in news articles, research papers, or social media.\n",
    "   - **Example:** Analyzing Reddit discussions to identify trending topics.\n",
    "2. **Document Classification & Tagging**\n",
    "   - Documents can be classified into multiple categories based on their topic mixture.\n",
    "   - **Example:** A tech article might be 30% AI, 40% cybersecurity, 30% blockchain.\n",
    "3. **Information Retrieval & Search**\n",
    "   - Instead of keyword matching, search engines can retrieve topic-related documents.\n",
    "   - **Example:** A search for \"deep learning\" may return AI-related articles.\n",
    "4. **Personalized Recommendations**\n",
    "   - User preferences can be modeled as topic distributions.\n",
    "   - **Example:** Netflix recommending shows based on a user's topic interests.\n",
    "\n",
    "# Nearest Neighbor Search & Retrieval\n",
    "\n",
    "## Nearest Neighbor Search (NNS)\n",
    "\n",
    "- **One-nearest neighbor (1-NN):** Finds the most similar data point.\n",
    "- **K-nearest neighbors (K-NN):** Returns the K most similar points.\n",
    "\n",
    "### Key Challenges & Solutions\n",
    "\n",
    "#### Data Representation\n",
    "\n",
    "- **TF-IDF for text retrieval:** Balances frequency and rarity.\n",
    "- **Feature weighting:** Title vs. abstract importance in documents.\n",
    "\n",
    "#### Distance Metrics\n",
    "\n",
    "- **Cosine Similarity:** Good for text, ignores magnitude.\n",
    "- **Euclidean Distance:** Used when raw magnitude matters.\n",
    "- **Scaled Euclidean Distance:** Adjusts importance of features.\n",
    "\n",
    "#### Scalability Issues\n",
    "\n",
    "- **Brute-force search:** $ O(N) $ per query, too slow for large datasets.\n",
    "- **KD-Trees:** Good for low-dimensional data.\n",
    "- **Locality-Sensitive Hashing (LSH):** For high-dimensional spaces.\n",
    "\n",
    "### Efficient Retrieval Techniques\n",
    "\n",
    "#### KD-Trees\n",
    "\n",
    "- Partition space hierarchically.\n",
    "- Prune large sections of space to speed up search.\n",
    "- Struggles with high-dimensional data.\n",
    "\n",
    "#### Locality-Sensitive Hashing (LSH)\n",
    "\n",
    "- Randomly project data points into buckets.\n",
    "- Finds approximate nearest neighbors faster than KD-Trees.\n",
    "- Useful for high-dimensional data (text, images, embeddings).\n",
    "\n",
    "# Clustering Algorithms\n",
    "\n",
    "## 1. K-Means Clustering\n",
    "\n",
    "Most widely used hard clustering algorithm.\n",
    "\n",
    "### Iterative Algorithm\n",
    "\n",
    "- **Assignment Step:** Assign points to the nearest cluster center.\n",
    "- **Update Step:** Recompute cluster centers.\n",
    "- Converges to a local minimum, initialization matters.\n",
    "- **Weakness:** Assumes spherical clusters.\n",
    "\n",
    "## 2. Gaussian Mixture Models (GMMs)\n",
    "\n",
    "Soft clustering alternative to K-means.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- Each cluster is a multivariate Gaussian.\n",
    "- Uses Expectation-Maximization (EM) algorithm:\n",
    "  - **E-step:** Compute probabilities of cluster membership.\n",
    "  - **M-step:** Update cluster parameters.\n",
    "- Handles overlapping clusters better than K-means.\n",
    "\n",
    "## 3. Probabilistic Clustering: Mixture Models\n",
    "\n",
    "- Generalization of GMMs.\n",
    "- Can model complex data distributions.\n",
    "- Used in document clustering, image segmentation, and anomaly detection.\n",
    "\n",
    "## 4. Latent Dirichlet Allocation (LDA) – Topic Modeling\n",
    "\n",
    "### Why Mixed Membership Models?\n",
    "\n",
    "- Clustering assumes one label per document, but real-world documents belong to multiple topics.\n",
    "- LDA assigns multiple topics per document with different probabilities.\n",
    "\n",
    "### LDA Components\n",
    "\n",
    "- **Topic-word distributions:** Each topic is a probability distribution over words.\n",
    "- **Document-topic distributions:** Each document has a distribution over topics.\n",
    "- **Word-topic assignments:** Each word is assigned to a topic.\n",
    "\n",
    "### LDA Inference: Gibbs Sampling\n",
    "\n",
    "- Iteratively reassigns words to topics based on:\n",
    "  - How prevalent the topic is in the document.\n",
    "  - How likely the word is under the topic.\n",
    "- Collapsed Gibbs Sampling marginalizes out parameters, reducing complexity.\n",
    "\n",
    "### Applications\n",
    "\n",
    "- **News categorization:** Assign articles to multiple topics.\n",
    "- **Recommendation systems:** Learn user preferences from topic proportions.\n",
    "- **Search engines:** Retrieve documents based on topic similarity.\n",
    "\n",
    "## 5. Hierarchical Clustering\n",
    "\n",
    "### Why Use Hierarchical Clustering?\n",
    "\n",
    "- No need to specify number of clusters.\n",
    "- Creates a hierarchy (dendrogram), allowing clusters at different granularities.\n",
    "- Can capture complex cluster shapes.\n",
    "\n",
    "### Two Main Types\n",
    "\n",
    "#### Divisive Clustering (Top-Down)\n",
    "\n",
    "- Start with one large cluster, recursively split.\n",
    "- **Example:** Recursive K-Means.\n",
    "\n",
    "#### Agglomerative Clustering (Bottom-Up)\n",
    "\n",
    "- Start with each point as its own cluster.\n",
    "- Merge closest clusters iteratively.\n",
    "\n",
    "### Linkage Methods\n",
    "\n",
    "- **Single Linkage:** Merge clusters based on minimum pairwise distance.\n",
    "- **Complete Linkage:** Merge based on maximum pairwise distance.\n",
    "- **Ward's Method:** Minimizes variance within clusters (good for balanced clusters).\n",
    "\n",
    "### Cutting the Dendrogram\n",
    "\n",
    "- Choose threshold (D) to determine clusters.\n",
    "- Application-Specific: Small D → more clusters, large D → fewer clusters.\n",
    "\n",
    "## 6. Hidden Markov Models (HMMs)\n",
    "\n",
    "### Why Use HMMs?\n",
    "\n",
    "- Standard clustering ignores sequential dependencies.\n",
    "- HMMs model dynamic clustering where the current state depends on the previous state.\n",
    "\n",
    "### HMM Components\n",
    "\n",
    "- **Hidden States (Clusters):** Underlying structure (e.g., dance moves of bees 🐝).\n",
    "- **Emission Probabilities:** Probability of observing a given value from a state.\n",
    "- **Transition Probabilities:** Probability of moving between states.\n",
    "\n",
    "### Inference in HMMs\n",
    "\n",
    "- **Baum-Welch Algorithm (EM for HMMs):** Learns model parameters.\n",
    "- **Viterbi Algorithm:** Finds most probable state sequence.\n",
    "- **Forward-Backward Algorithm:** Computes soft assignments.\n",
    "\n",
    "### Applications\n",
    "\n",
    "- **Speech recognition:** Segmenting audio into phonemes.\n",
    "- **Stock market analysis:** Identifying market regimes.\n",
    "- **Biological sequence modeling:** DNA/protein sequence analysis.\n",
    "\n",
    "\n",
    "# Vector Spaces in NLP\n",
    "\n",
    "Understanding vector representations is crucial for many NLP tasks, from word similarity to machine translation. This module introduces vector spaces in NLP, focusing on how words and documents can be represented as numerical vectors.\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "- [Vector Spaces in NLP](#vector-spaces-in-nlp)\n",
    "  - [Why Do We Need Vector Spaces?](#why-do-we-need-vector-spaces)\n",
    "  - [How to Construct Word Vectors?](#how-to-construct-word-vectors)\n",
    "  - [Measuring Word and Document Similarity](#measuring-word-and-document-similarity)\n",
    "  - [Word Arithmetic: Using Vectors for Meaning](#word-arithmetic-using-vectors-for-meaning)\n",
    "  - [Dimensionality Reduction: PCA for Visualizing Word Vectors](#dimensionality-reduction-pca-for-visualizing-word-vectors)\n",
    "- [Word Translation with Word Vectors](#word-translation-with-word-vectors)\n",
    "  - [How Can a Machine Learn to Translate?](#how-can-a-machine-learn-to-translate)\n",
    "  - [Finding the Transformation Matrix (R)](#finding-the-transformation-matrix-r)\n",
    "  - [Finding Similar Word Vectors with Nearest Neighbors](#finding-similar-word-vectors-with-nearest-neighbors)\n",
    "  - [Locality-Sensitive Hashing (LSH) for Faster Search](#locality-sensitive-hashing-lsh-for-faster-search)\n",
    "  - [Document Search with LSH](#document-search-with-lsh)\n",
    "\n",
    "## Why Do We Need Vector Spaces?\n",
    "\n",
    "Words can have similar meanings even if they don’t share the same characters.\n",
    "\n",
    "### Example:\n",
    "- **\"Where are you heading?\"** vs. **\"Where are you from?\"** → Different meaning despite similar words.\n",
    "- **\"What’s your location?\"** vs. **\"Where are you?\"** → Similar meaning despite different words.\n",
    "\n",
    "Vector spaces help quantify these relationships by encoding words as numbers.\n",
    "\n",
    "### Applications of Vector Spaces in NLP:\n",
    "- **Text similarity:** Helps in question answering, paraphrasing, summarization.\n",
    "- **Semantic relationships:** Detects word dependencies (e.g., \"cereal\" and \"bowl\" are related).\n",
    "- **Information retrieval:** Search engines rank documents based on vector representations.\n",
    "\n",
    "🚀 **Famous NLP Principle:** “You shall know a word by the company it keeps” — John Firth\n",
    "\n",
    "(i.e., words appearing in similar contexts tend to have similar meanings.)\n",
    "\n",
    "---\n",
    "\n",
    "## How to Construct Word Vectors?\n",
    "\n",
    "There are two common ways to build vector space representations:\n",
    "\n",
    "### Co-Occurrence Matrices (Word-by-Word Design)\n",
    "- Measures how often words appear together within a fixed distance.\n",
    "- **Example:**\n",
    "  - Corpus: \"Data science is simple. Raw data is useful.\"\n",
    "  - Word: \"data\"\n",
    "  - If \"data\" appears within 2 words of \"simple\", the count is updated.\n",
    "  - Each word is represented as a vector of co-occurrence counts.\n",
    "\n",
    "### Word-by-Document Matrix\n",
    "- Instead of words, we track how often words appear in documents (topic-based).\n",
    "- **Example:**\n",
    "  - Word: \"data\"\n",
    "  - Occurrences: 500 times in Entertainment, 6620 in Economy, 9320 in Machine Learning.\n",
    "  - Words like \"film\" may have high frequency in Entertainment but low in Economy.\n",
    "\n",
    "🚀 **Key Insight:**\n",
    "- Vector representations allow document clustering → Similar topics appear closer in vector space.\n",
    "\n",
    "---\n",
    "\n",
    "## Measuring Word and Document Similarity\n",
    "\n",
    "Once words are transformed into vectors, we need a way to compare their similarity.\n",
    "\n",
    "### Euclidean Distance (Straight-Line Distance)\n",
    "- Measures how far apart two vectors are.\n",
    "- **Formula:**\n",
    "  $\n",
    "  d(A, B) = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\n",
    "  $\n",
    "- **Example:**\n",
    "  - \"Entertainment\" and \"Machine Learning\" documents have a Euclidean distance of 10,667.\n",
    "\n",
    "**Issue:**\n",
    "- Does not account for vector magnitude (size differences).\n",
    "- A long article vs. a short article about the same topic might be far apart.\n",
    "\n",
    "### Cosine Similarity (Angle-Based Similarity)\n",
    "- More effective than Euclidean Distance for comparing word/document vectors.\n",
    "- **Formula:**\n",
    "  $\n",
    "  \\text{cosine similarity} = \\frac{A \\cdot B}{||A|| \\times ||B||}\n",
    "  $\n",
    "- If cosine similarity ≈ 1, vectors are very similar. If ≈ 0, they are unrelated.\n",
    "- **Example:**\n",
    "  - \"Agriculture\" and \"History\" might be far apart (large Euclidean distance) but similar (small cosine angle).\n",
    "\n",
    "🚀 **Key Takeaways:**\n",
    "- **Euclidean Distance** is good for small-scale problems.\n",
    "- **Cosine Similarity** is better for comparing large documents or word embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "## Word Arithmetic: Using Vectors for Meaning\n",
    "\n",
    "You can manipulate word vectors using basic arithmetic.\n",
    "\n",
    "### Example:\n",
    "- **\"USA\" + \"Washington DC\"** → Produces a vector difference that represents the relationship between a country and its capital.\n",
    "- If we apply the same difference to **\"Russia\"**, we should get **\"Moscow\"**.\n",
    "\n",
    "🚀 **Word Analogies Using Vectors**\n",
    "\n",
    "- **\"King\" - \"Man\" + \"Woman\" ≈ \"Queen\"**\n",
    "- **\"Paris\" - \"France\" + \"Germany\" ≈ \"Berlin\"**\n",
    "\n",
    "---\n",
    "\n",
    "## Dimensionality Reduction: PCA for Visualizing Word Vectors\n",
    "\n",
    "Word vectors are high-dimensional (hundreds of dimensions). Principal Component Analysis (PCA) helps reduce dimensions while preserving important relationships.\n",
    "\n",
    "### Example:\n",
    "- Words like \"city\" and \"town\" or \"oil\" and \"gas\" appear closer together in lower dimensions.\n",
    "\n",
    "### PCA Uses:\n",
    "- **Eigenvalues and Eigenvectors** to extract important features.\n",
    "- Projects word vectors into a **2D or 3D space** for visualization.\n",
    "\n",
    "🚀 **Why Use PCA?**\n",
    "- Helps understand relationships between words.\n",
    "- Reduces computational cost in NLP tasks.\n",
    "\n",
    "# Word Translation with Word Vectors\n",
    "\n",
    "One of the fundamental applications of NLP is machine translation. The goal is to translate an English word (e.g., **hello**) to its French equivalent (**bonjour**) using word embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "## How Can a Machine Learn to Translate?\n",
    "\n",
    "1. Create word vectors for both English and French words.\n",
    "2. Find a transformation (matrix **R**) that maps English vectors to French vectors.\n",
    "3. Search for the nearest vector in the French space to get the translation.\n",
    "\n",
    "### The Role of Word Embeddings\n",
    "\n",
    "- English and French word embeddings exist in different vector spaces.\n",
    "- The goal is to find a transformation (**R matrix**) to align these spaces.\n",
    "- Instead of creating a hardcoded dictionary, we can train the model on a small word set and generalize to unseen words.\n",
    "\n",
    "### Finding the Transformation Matrix (R)\n",
    "\n",
    "Given:\n",
    "- **X** = matrix of English word embeddings.\n",
    "- **Y** = matrix of French word embeddings.\n",
    "\n",
    "We optimize **R** using Gradient Descent to minimize:\n",
    "\n",
    "$\n",
    "||XR - Y||_F^2\n",
    "$\n",
    "\n",
    "(Frobenius norm): Measures the difference between transformed English words and actual French words.\n",
    "\n",
    "- **Updating R:** Uses gradient descent to minimize this difference.\n",
    "\n",
    "🚀 **Key Insight:** By learning **R**, we can translate words without needing a direct dictionary!\n",
    "\n",
    "---\n",
    "\n",
    "## Finding Similar Word Vectors with Nearest Neighbors\n",
    "\n",
    "Once we have transformed the English vector into the French space, we need to find the closest French word vector.\n",
    "\n",
    "### Brute Force K-Nearest Neighbors (KNN)\n",
    "\n",
    "- Given a word vector, compare it with all words in the French space.\n",
    "- Find the **k** closest words using **cosine similarity** or **Euclidean distance**.\n",
    "\n",
    "💡 **Problem?**\n",
    "- If we have millions of words, this brute-force search is slow.\n",
    "\n",
    "### A Faster Solution: Hash Tables\n",
    "\n",
    "- Instead of searching through every word vector, we divide vectors into **buckets**.\n",
    "- Similar vectors get placed into the same bucket.\n",
    "- Hashing allows us to search efficiently within a smaller set.\n",
    "\n",
    "---\n",
    "\n",
    "## Locality-Sensitive Hashing (LSH) for Faster Search\n",
    "\n",
    "To speed up nearest neighbor search, we use **Locality-Sensitive Hashing (LSH)**.\n",
    "\n",
    "### What is Hashing?\n",
    "\n",
    "- Think of a cupboard with drawers.\n",
    "- You store similar items in the same drawer.\n",
    "- Instead of searching through everything, you only check one drawer.\n",
    "\n",
    "### Basic Hashing\n",
    "\n",
    "- Each word vector is assigned a **hash value** based on a hash function.\n",
    "- **Example hash function:**\n",
    "\n",
    "$\n",
    "\\text{Hash Value} = \\text{Word Vector} \\mod 10\n",
    "$\n",
    "\n",
    "💡 **Problem?**\n",
    "- This simple method doesn’t guarantee that similar words end up in the same bucket.\n",
    "\n",
    "---\n",
    "\n",
    "## Locality-Sensitive Hashing (LSH)\n",
    "\n",
    "LSH fixes the problem of basic hashing by ensuring that similar words get hashed to the same bucket.\n",
    "\n",
    "### How Does LSH Work?\n",
    "\n",
    "1. Divide the vector space using **random hyperplanes** (splitting lines).\n",
    "2. Assign hash values based on which side of the hyperplane the vector falls.\n",
    "3. Similar words will likely fall into the same bucket.\n",
    "\n",
    "🚀 **Example: Finding Nearby Friends**\n",
    "\n",
    "- Imagine you are visiting San Francisco.\n",
    "- Instead of checking every friend in the world, you only check the ones in the USA.\n",
    "- **LSH does the same thing for words**—it reduces the search space to relevant groups.\n",
    "\n",
    "### Mathematical Foundation of LSH\n",
    "\n",
    "- Each **hyperplane** divides the space into two regions.\n",
    "- Compute **dot product** between the word vector and the normal vector of the hyperplane.\n",
    "  - **Positive dot product** → One side of the plane.\n",
    "  - **Negative dot product** → Other side of the plane.\n",
    "- Assign **binary hash values** (0 or 1) based on sign.\n",
    "- Combine multiple hash values to get a **unique region (bucket)**.\n",
    "\n",
    "### Why Use LSH?\n",
    "\n",
    "✅ Much faster than brute-force search.\n",
    "✅ Efficiently finds **approximate nearest neighbors**.\n",
    "✅ Trade-off between accuracy and speed.\n",
    "\n",
    "---\n",
    "\n",
    "## Document Search with LSH\n",
    "\n",
    "Another application of nearest neighbor search is **document retrieval**.\n",
    "\n",
    "### How to Represent Documents as Vectors?\n",
    "\n",
    "- Words have **vector representations**.\n",
    "- A document can be represented as **the sum of its word vectors**.\n",
    "\n",
    "### Finding Similar Documents\n",
    "\n",
    "**Example Query:** \"Can I get a refund?\"\n",
    "\n",
    "1. Convert the query into a **document vector**.\n",
    "2. Find the **nearest documents** using LSH.\n",
    "\n",
    "**Possible matches:**\n",
    "- \"What’s your return policy?\"\n",
    "- \"May I get my money back?\"\n",
    "\n",
    "🚀 **Why is this useful?**\n",
    "\n",
    "- LSH allows **fast document retrieval** for search engines, chatbots, and Q&A systems.\n",
    "\n",
    "\n",
    "# Introduction to GenAI\n",
    "\n",
    "# Introduction to Generative AI\n",
    "\n",
    "## 1. Understanding AI, Machine Learning & Deep Learning\n",
    "\n",
    "Before diving into Generative AI (GenAI), the course first explains the fundamentals of Artificial Intelligence (AI) and Machine Learning (ML):\n",
    "\n",
    "- **AI** is a branch of computer science that creates intelligent systems that can reason, learn, and act autonomously.\n",
    "- **Machine Learning (ML)** is a subset of AI where models learn from data instead of being explicitly programmed.\n",
    "\n",
    "### Types of ML models:\n",
    "\n",
    "- **Supervised Learning:** Uses labeled data to train models (e.g., predicting tips based on past restaurant bills).\n",
    "- **Unsupervised Learning:** Works with unlabeled data to find hidden patterns (e.g., clustering employees based on tenure and income).\n",
    "\n",
    "## 2. Deep Learning & Neural Networks\n",
    "\n",
    "- **Deep Learning (DL)** is a subset of ML that uses artificial neural networks to process complex patterns in data.\n",
    "- **Neural Networks** mimic the human brain with interconnected nodes (neurons).\n",
    "- **Semi-supervised learning:** Uses a mix of labeled and unlabeled data to train models.\n",
    "\n",
    "## 3. Where Does Generative AI Fit In?\n",
    "\n",
    "- **Generative AI** is a subset of Deep Learning, meaning it leverages artificial neural networks.\n",
    "- Generative AI differs from traditional ML:\n",
    "  - **Discriminative models** classify data points (e.g., is this image a dog or cat?).\n",
    "  - **Generative models** learn the probability distribution of data and generate new data points (e.g., creating an entirely new image of a dog).\n",
    "\n",
    "## 4. What Can Generative AI Do?\n",
    "\n",
    "Generative AI creates new content rather than just classifying existing data. Some examples include:\n",
    "\n",
    "- **Text Generation** (e.g., Chatbots, LLMs like Gemini and LaMDA)\n",
    "- **Image Generation** (e.g., Stable Diffusion)\n",
    "- **Audio and Video Generation** (e.g., AI-generated music or deepfake videos)\n",
    "- **Code Generation** (e.g., AI-assisted programming)\n",
    "- **3D Model Generation** (e.g., creating 3D assets for video games)\n",
    "\n",
    "## 5. Generative AI vs Traditional ML\n",
    "\n",
    "A traditional ML model predicts an output (e.g., a number or class label), whereas a Generative AI model learns patterns and creates new content.\n",
    "\n",
    "- **Example:** If input = “What is sales?”, a traditional ML model might predict a probability (e.g., likelihood of a sale), while a GenAI model would generate a full-text definition.\n",
    "\n",
    "## 6. Large Language Models (LLMs)\n",
    "\n",
    "LLMs are a subset of Generative AI that process text-based inputs and generate human-like text.\n",
    "\n",
    "### How do they work?\n",
    "\n",
    "- Given an input prompt, they predict the next word based on probabilities from training data.\n",
    "- **Example:** “I’m making a sandwich with peanut butter and…” → likely response: “jelly”.\n",
    "\n",
    "## 7. The Role of Transformers in Generative AI\n",
    "\n",
    "Transformers revolutionized Natural Language Processing (NLP) in 2018.\n",
    "\n",
    "- A transformer consists of an encoder-decoder structure:\n",
    "  - The encoder processes the input.\n",
    "  - The decoder generates the output.\n",
    "- While powerful, transformers can hallucinate (generate incorrect or nonsensical content).\n",
    "\n",
    "## 8. Prompt Engineering: Controlling AI Outputs\n",
    "\n",
    "- A prompt is the input text given to an AI model to guide its output.\n",
    "- Prompt design is crucial to obtaining high-quality AI responses.\n",
    "\n",
    "## 9. Types of Generative AI Models\n",
    "\n",
    "Different models serve different input-output functions:\n",
    "\n",
    "- **Text-to-Text:** Language translation, summarization (e.g., ChatGPT, Gemini).\n",
    "- **Text-to-Image:** Generates images from text prompts (e.g., DALL·E, Stable Diffusion).\n",
    "- **Text-to-Video:** Generates videos from text prompts.\n",
    "- **Text-to-3D:** Creates 3D models from text descriptions.\n",
    "- **Text-to-Task:** Automates workflows, assists with web navigation, or modifies documents.\n",
    "\n",
    "## 10. Foundation Models: The Backbone of Generative AI\n",
    "\n",
    "Foundation models are large AI models pre-trained on massive datasets and can be fine-tuned for specific applications.\n",
    "\n",
    "### Examples:\n",
    "\n",
    "- **LLMs** (e.g., Gemini, LaMDA)\n",
    "- **Vision models** (e.g., Stable Diffusion for image generation)\n",
    "- **Code models** (e.g., AI-assisted coding, debugging, and SQL query generation)\n",
    "\n",
    "## 11. Google Cloud’s Generative AI Tools\n",
    "\n",
    "Google Cloud offers tools for developers and businesses:\n",
    "\n",
    "- **Vertex AI Studio:** Helps developers build and deploy Generative AI models.\n",
    "- **Vertex AI Agent Builder:** Enables low-code/no-code AI solutions for chatbots, digital assistants, and knowledge bases.\n",
    "- **Gemini AI:** A multimodal model capable of handling text, images, audio, and code.\n",
    "\n",
    "## 12. Applications of Generative AI\n",
    "\n",
    "Generative AI is being used in many industries:\n",
    "\n",
    "- **Healthcare:** Drug discovery, patient diagnosis.\n",
    "- **Finance:** Fraud detection, risk assessment.\n",
    "- **Customer Support:** Chatbots, automated responses.\n",
    "- **Education:** Personalized learning content.\n",
    "- **Entertainment & Creativity:** AI-generated music, writing, and gaming assets.\n",
    "\n",
    "# Introduction to LLMs\n",
    "\n",
    "# Introduction to Large Language Models (LLMs)\n",
    "\n",
    "LLMs are a subset of deep learning that focus on understanding, processing, and generating human language at scale. These models, such as GPT, LaMDA, and Gemini, have revolutionized Natural Language Processing (NLP) by pre-training on massive datasets and later being fine-tuned for specific tasks.\n",
    "\n",
    "## 1. What Makes LLMs “Large”?\n",
    "\n",
    "The term \"large\" refers to:\n",
    "\n",
    "- **Training Data:** Typically in petabyte scale.\n",
    "- **Parameters:** These are learned weights in deep learning models, often in billions or even trillions, making the model more knowledgeable and powerful.\n",
    "\n",
    "## 2. How LLMs Work\n",
    "\n",
    "LLMs operate in two stages:\n",
    "\n",
    "- **Pre-training:** Models learn general language structures using large, unlabeled datasets.\n",
    "- **Fine-tuning:** Models are customized for specific tasks with domain-specific data.\n",
    "\n",
    "**Analogy:** Pre-training is like teaching a dog basic commands, and fine-tuning is specialized training (e.g., for police or guide dogs).\n",
    "\n",
    "## 3. Key Benefits of LLMs\n",
    "\n",
    "- **Versatility:** A single model can perform multiple tasks like:\n",
    "  - Language Translation\n",
    "  - Question Answering (QA)\n",
    "  - Text Summarization\n",
    "  - Sentiment Analysis\n",
    "- **Few-Shot & Zero-Shot Learning:** LLMs perform well with minimal training data.\n",
    "- **Scalability:** Performance improves as more data and parameters are added.\n",
    "\n",
    "## 4. Underlying Technology: Transformers\n",
    "\n",
    "LLMs are Transformer-based architectures, consisting of:\n",
    "\n",
    "- **Encoders:** For input representation.\n",
    "- **Decoders:** For generating outputs.\n",
    "\n",
    "Transformers leverage self-attention mechanisms, which allow them to capture long-range dependencies in text better than traditional RNNs or CNNs.\n",
    "\n",
    "## Prompting: The New \"Programming\" Paradigm\n",
    "\n",
    "Instead of traditional model training, users interact with LLMs through prompting.\n",
    "\n",
    "### 1. Prompt Design vs. Prompt Engineering\n",
    "\n",
    "- **Prompt Design:** Writing clear and concise instructions for an AI model.\n",
    "- **Prompt Engineering:** Optimizing prompts to improve accuracy and efficiency.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- Basic Prompt: \"Translate this to French: ‘Hello, how are you?’\"\n",
    "- Engineered Prompt: \"Translate the following English sentence to formal French, considering polite expressions: ‘Hello, how are you?’\"\n",
    "\n",
    "### 2. Types of LLMs Based on Prompting\n",
    "\n",
    "- **Generic LLMs:** Predict next words based on context (like autocomplete).\n",
    "- **Instruction-Tuned LLMs:** Follow specific instructions (e.g., summarization, classification).\n",
    "- **Dialog-Tuned LLMs:** Trained for conversational AI, providing context-aware responses.\n",
    "\n",
    "### 3. Chain-of-Thought Reasoning\n",
    "\n",
    "Instead of answering directly, models break down complex problems step-by-step before arriving at a final answer.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "**Question:** Roger has 5 tennis balls. He buys 2 cans, each containing 3 balls. How many does he have?\n",
    "\n",
    "**Chain-of-Thought Output:**\n",
    "\n",
    "- \"Roger starts with 5 balls.\"\n",
    "- \"Each can has 3 balls. He buys 2 cans (3 × 2 = 6).\"\n",
    "- \"Total balls = 5 + 6 = 11 balls.\"\n",
    "\n",
    "This approach significantly improves accuracy.\n",
    "\n",
    "## Tuning & Customization of LLMs\n",
    "\n",
    "- **Fine-Tuning:** Training on domain-specific data (e.g., healthcare, legal texts).\n",
    "- **Parameter-Efficient Tuning (PETM):** Instead of retraining the entire model, only small adapter layers are fine-tuned to reduce costs.\n",
    "- **Task-Specific Models:** LLMs can be tailored for:\n",
    "  - Sentiment Analysis\n",
    "  - Image Recognition\n",
    "  - Occupancy Analytics\n",
    "\n",
    "## Google Cloud’s LLM Ecosystem\n",
    "\n",
    "### 1. Vertex AI Studio\n",
    "\n",
    "- Enables quick exploration and customization of LLMs.\n",
    "- Provides pre-trained models, fine-tuning tools, and production deployment support.\n",
    "\n",
    "### 2. Vertex AI Agent Builder\n",
    "\n",
    "- A low-code/no-code platform to build chatbots, virtual assistants, and search engines.\n",
    "\n",
    "### 3. Gemini: Google’s Multimodal AI\n",
    "\n",
    "- Unlike text-based models, Gemini processes text, images, audio, and even code.\n",
    "\n",
    "### 4. Model Garden\n",
    "\n",
    "- A constantly updated repository of AI models, offering customization options.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
